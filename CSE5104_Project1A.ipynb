{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "022a1013-595b-40d9-a86c-711b397d3126",
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTS\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7bc2478-cec1-4e83-81da-80b1f7370abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTIONS\n",
    "\n",
    "## Min-Max Normalization\n",
    "# X: array to be normalized\n",
    "def normalize(X):\n",
    "    X_norm = (X-np.min(X, axis=0))/(np.max(X, axis=0) - np.min(X, axis=0))\n",
    "    return X_norm\n",
    "\n",
    "## Univariate Gradient Descent\n",
    "# X: feature vector\n",
    "# y: target vector\n",
    "# alpha: step size\n",
    "# max_iter: number of iterations \n",
    "def uni_gradient_descent(X, y, alpha, max_iter):\n",
    "    m = 1\n",
    "    b = 1\n",
    "    n = len(X)\n",
    "    for i in range(max_iter):\n",
    "        m_gradient = np.mean(-2*X*(y - (m*X + b)))\n",
    "        b_gradient = np.mean(-2*(y - (m*X + b)))\n",
    "        m = m - alpha*m_gradient\n",
    "        b = b - alpha*b_gradient\n",
    "    return m, b\n",
    "\n",
    "### Multivariate Gradient Descent\n",
    "# X: feature array \n",
    "# y: target vector\n",
    "# alpha: step size\n",
    "# max_iter: number of iterations \n",
    "def multi_gradient_descent(X, y, alpha, max_iter):\n",
    "    if (X.ndim == 1):\n",
    "        X = X.reshape(1, -1)\n",
    "    n, num_feats = X.shape\n",
    "    m = np.ones((num_feats,1))\n",
    "    b = 1\n",
    "    for i in range(max_iter):\n",
    "        m_gradient = np.sum(-2*(y - (X@m + b))*X, axis=0).reshape(1, -1).transpose()\n",
    "        b_gradient = np.sum(-2*(y - (X@m + b)))\n",
    "        m = m - alpha*m_gradient/n\n",
    "        b = b - alpha*b_gradient/n\n",
    "    return m, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26d6f423-b7e2-4368-8f05-43b1323ee128",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREPROCESSING I\n",
    "\n",
    "# read data set\n",
    "data = pd.read_csv(r\"C:\\Users\\kingh\\Downloads\\Concrete_Data.csv\")\n",
    "\n",
    "# extract feature variables\n",
    "X = data.iloc[:,:8].to_numpy()\n",
    "X_test = X[501:631, :]\n",
    "X_train = np.vstack((X[0:501, :], X[631:, :]))\n",
    "\n",
    "# extract target variable\n",
    "y = data.iloc[:, -1].to_numpy().reshape((-1,1))\n",
    "y_test = y[501:631]\n",
    "y_train = np.vstack((y[0:501], y[631:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29ba3542-13b9-49b5-9b2b-821598a03399",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREPROCESSING II\n",
    "\n",
    "# create normalized X\n",
    "X_train_norm = normalize(X_train)\n",
    "X_test_norm = normalize(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c4d8fe-0870-45f7-b019-a493dc8da491",
   "metadata": {},
   "source": [
    "# UNIVARIATE LINEAR MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32b8ded-1bf9-4797-90ca-caa40c21a6d9",
   "metadata": {},
   "source": [
    "## NORMALIZED PREDICTORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e25fae3e-30b8-4cec-8d3b-886d639059ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: [0.004641588833612777, 129]\n",
      "Best Test VE: [ 0.18619992 -0.05296291  0.13085162 -0.11010294  0.14396512 -0.23920968\n",
      " -0.11501515  0.01860353]\n",
      "Corresponding Train VE: [-0.10267314 -0.32989611 -0.42359578 -0.33249195 -0.31981907 -0.30055846\n",
      " -0.32186009 -0.36360203]\n",
      "m: [[12.09544269]\n",
      " [ 7.46018168]\n",
      " [ 6.11105546]\n",
      " [10.40240548]\n",
      " [ 7.18534966]\n",
      " [11.12862652]\n",
      " [10.22574434]\n",
      " [ 5.13395483]]\n",
      "b: [[23.73218639]\n",
      " [25.21405681]\n",
      " [25.21129053]\n",
      " [23.6057197 ]\n",
      " [25.39393915]\n",
      " [23.25085355]\n",
      " [23.78495704]\n",
      " [25.84555499]]\n"
     ]
    }
   ],
   "source": [
    "### GRID SEARCH FOR OPTIMAL HYPERPARAMS (UNIVARIATE, NORMALIZED)\n",
    "\n",
    "alpha_values = np.logspace(-3, -1, 10)\n",
    "max_iter_values = np.logspace(1, 3, 10, dtype=int)\n",
    "best_m = None\n",
    "best_b = None\n",
    "best_VE_train = None\n",
    "best_params = None\n",
    "best_VE = None\n",
    "best_avg_VE = -np.inf\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    for max_iter in max_iter_values:\n",
    "        \n",
    "        # initialize parameters\n",
    "        num_feats = len(X_train_norm[0])          # number of features\n",
    "        \n",
    "        # conduct gradient descent to obtain a univariate linear regression model for each feature\n",
    "        m = np.zeros(num_feats)\n",
    "        b = np.zeros(num_feats)\n",
    "        for i in range(num_feats):\n",
    "            X_i = X_train_norm[:, i].reshape((-1,1))\n",
    "            m_i, b_i = uni_gradient_descent(X_i, y_train, alpha, max_iter)\n",
    "            m[i] = m_i\n",
    "            b[i] = b_i\n",
    "        \n",
    "        # evaluate variances\n",
    "        var_train = np.var(y_train)\n",
    "        var_test = np.var(y_test)\n",
    "        \n",
    "        # model predictions\n",
    "        m = m.reshape(-1,1)\n",
    "        b = b.reshape(-1,1)\n",
    "        y_train_pred = (X_train_norm.transpose()*m + b).transpose()\n",
    "        y_test_pred = (X_test_norm.transpose()*m + b).transpose()\n",
    "        \n",
    "        # train + test MSE\n",
    "        train_MSE = np.mean((y_train_pred - np.tile(y_train, (1, num_feats)))**2, axis=0)\n",
    "        test_MSE  = np.mean((y_test_pred - np.tile(y_test, (1, num_feats)))**2, axis=0)\n",
    "        \n",
    "        # train + test VE\n",
    "        train_VE = 1 - (train_MSE / var_train)\n",
    "        test_VE  = 1 - (test_MSE / var_test)\n",
    "        \n",
    "        # update best model based on average test VE\n",
    "        if np.mean(test_VE) > best_avg_VE:\n",
    "            best_avg_VE = np.mean(test_VE)\n",
    "            best_VE = test_VE\n",
    "            best_params = [alpha, max_iter]\n",
    "            best_m = m\n",
    "            best_b = b\n",
    "            best_VE_train = train_VE\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Test VE:\", best_VE)\n",
    "print(\"Corresponding Train VE:\", best_VE_train)\n",
    "print(\"m:\", best_m)\n",
    "print(\"b:\", best_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7c40111-c9bd-4a61-83aa-5fadd5b944f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m: [[12.56225204]\n",
      " [ 7.71724786]\n",
      " [ 6.2155356 ]\n",
      " [10.67385511]\n",
      " [ 7.46990294]\n",
      " [11.41878722]\n",
      " [10.50429539]\n",
      " [ 5.34664105]]\n",
      "b: [[24.54717439]\n",
      " [26.18380807]\n",
      " [26.19291599]\n",
      " [24.43239196]\n",
      " [26.37837713]\n",
      " [24.0447017 ]\n",
      " [24.62620564]\n",
      " [26.87523594]]\n",
      "Test VE: [ 0.17238119 -0.04683249  0.13978374 -0.12812279  0.15721965 -0.25756707\n",
      " -0.12567177  0.02813135]\n",
      "Train VE: [-0.04112086 -0.25962091 -0.35554444 -0.27968436 -0.24648718 -0.25074039\n",
      " -0.26707893 -0.28638303]\n",
      "Test MSE: [180.2050573  227.93646867 187.30280136 245.63655401 183.50632011\n",
      " 273.82164925 245.10287164 211.61390238]\n",
      "Train MSE: [288.47410752 349.0161713  375.59469483 354.57535927 345.3770724\n",
      " 346.55555477 351.08264145 356.43142684]\n"
     ]
    }
   ],
   "source": [
    "### TRAIN + ANALYZE UNIVARIATE MODELS (NORMALIZED)\n",
    "\n",
    "# initialize parameters\n",
    "num_feats = len(X_train_norm[0])          # number of features\n",
    "alpha = 0.005                              # learning rate       \n",
    "max_iter = 130                            # number of iterations\n",
    "\n",
    "# conduct gradient descent to obtain a univariate linear regression model for each feature\n",
    "m = np.zeros(num_feats)\n",
    "b = np.zeros(num_feats)\n",
    "for i in range(num_feats):\n",
    "    X_i = X_train_norm[:, i].reshape((-1,1))\n",
    "    m_i, b_i = uni_gradient_descent(X_i, y_train, alpha, max_iter)\n",
    "    m[i] = m_i\n",
    "    b[i] = b_i\n",
    "\n",
    "# evaluate variances\n",
    "var_train = np.var(y_train)\n",
    "var_test = np.var(y_test)\n",
    "\n",
    "# model predictions\n",
    "m = m.reshape(-1,1)\n",
    "b = b.reshape(-1,1)\n",
    "y_train_pred = (X_train_norm.transpose()*m + b).transpose()\n",
    "y_test_pred = (X_test_norm.transpose()*m + b).transpose()\n",
    "\n",
    "# train + test MSE\n",
    "train_MSE = np.mean((y_train_pred - np.tile(y_train, (1, num_feats)))**2, axis=0)\n",
    "test_MSE  = np.mean((y_test_pred - np.tile(y_test, (1, num_feats)))**2, axis=0)\n",
    "\n",
    "# train + test VE\n",
    "train_VE = 1 - (train_MSE / var_train)\n",
    "test_VE  = 1 - (test_MSE / var_test)\n",
    "\n",
    "print(\"m:\", m)\n",
    "print(\"b:\", b)\n",
    "print(\"Test VE:\", test_VE)\n",
    "print(\"Train VE:\", train_VE)\n",
    "print(\"Test MSE:\", test_MSE)\n",
    "print(\"Train MSE:\", train_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbce214f-ddb6-484c-9d16-b608b846f575",
   "metadata": {},
   "source": [
    "## RAW PREDICTORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "9bbd804e-7b39-468f-b7b3-23898055da03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kingh\\AppData\\Local\\Temp\\ipykernel_32256\\2824116174.py:37: RuntimeWarning: overflow encountered in square\n",
      "  train_MSE = np.mean((y_train_pred - np.tile(y_train, (1, num_feats)))**2, axis=0)\n",
      "C:\\Users\\kingh\\AppData\\Local\\Temp\\ipykernel_32256\\2824116174.py:38: RuntimeWarning: overflow encountered in square\n",
      "  test_MSE  = np.mean((y_test_pred - np.tile(y_test, (1, num_feats)))**2, axis=0)\n",
      "C:\\Users\\kingh\\AppData\\Local\\Temp\\ipykernel_32256\\1696712276.py:21: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  m = m - alpha*m_gradient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.2023126630627305\n",
      "Best Parameters: [0.0001, 1000]\n",
      "Best Test VE: [        nan        -inf -1.04135567         nan -1.35068735         nan\n",
      "         nan -2.20231266]\n",
      "Corresponding Train VE: [        nan        -inf -2.40337679         nan -1.05602793         nan\n",
      "         nan -1.66191696]\n",
      "m: [[            nan]\n",
      " [1.10562839e+196]\n",
      " [2.17041825e-001]\n",
      " [            nan]\n",
      " [3.15501148e+000]\n",
      " [            nan]\n",
      " [            nan]\n",
      " [3.12945023e-001]]\n",
      "b: [[            nan]\n",
      " [6.54918240e+193]\n",
      " [5.09634067e+000]\n",
      " [            nan]\n",
      " [3.63294892e+000]\n",
      " [            nan]\n",
      " [            nan]\n",
      " [4.78737552e+000]]\n"
     ]
    }
   ],
   "source": [
    "### GRID SEARCH FOR OPTIMAL HYPERPARAMS (UNIVARIATE, RAW)\n",
    "\n",
    "alpha_values = [0.0001, 0.00001]\n",
    "max_iter_values = np.logspace(1, 3, 5, dtype=int)\n",
    "best_m = None\n",
    "best_b = None\n",
    "best_VE_train = None\n",
    "best_params = None\n",
    "best_VE = None\n",
    "best_avg_VE = -np.inf\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    for max_iter in max_iter_values:\n",
    "        # initialize parameters\n",
    "        num_feats = len(X_train[0])          # number of features\n",
    "        \n",
    "        # conduct gradient descent to obtain a univariate linear regression model for each feature\n",
    "        m = np.zeros(num_feats)\n",
    "        b = np.zeros(num_feats)\n",
    "        for i in range(num_feats):\n",
    "            X_i = X_train[:, i].reshape((-1,1))\n",
    "            m_i, b_i = uni_gradient_descent(X_i, y_train, alpha, max_iter)\n",
    "            m[i] = m_i\n",
    "            b[i] = b_i\n",
    "        \n",
    "        # evaluate variances\n",
    "        var_train = np.var(y_train)\n",
    "        var_test = np.var(y_test)\n",
    "        \n",
    "        # model predictions\n",
    "        m = m.reshape(-1,1)\n",
    "        b = b.reshape(-1,1)\n",
    "        y_train_pred = (X_train.transpose()*m + b).transpose()\n",
    "        y_test_pred = (X_test.transpose()*m + b).transpose()\n",
    "        \n",
    "        # train + test MSE\n",
    "        train_MSE = np.mean((y_train_pred - np.tile(y_train, (1, num_feats)))**2, axis=0)\n",
    "        test_MSE  = np.mean((y_test_pred - np.tile(y_test, (1, num_feats)))**2, axis=0)\n",
    "                \n",
    "        # train + test VE\n",
    "        train_VE = 1 - (train_MSE / var_train)\n",
    "        test_VE  = 1 - (test_MSE / var_test)\n",
    "        if (test_VE[7]+train_VE[7])/2 > best_avg_VE:\n",
    "            best_avg_VE = test_VE[7]\n",
    "            best_VE = test_VE\n",
    "            best_params = [alpha, max_iter]\n",
    "            best_m = m\n",
    "            best_b = b\n",
    "            best_VE_train = train_VE\n",
    "\n",
    "print(best_avg_VE)\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Test VE:\", best_VE)\n",
    "print(\"Corresponding Train VE:\", best_VE_train)\n",
    "print(\"m:\", best_m)\n",
    "print(\"b:\", best_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "e5ebe574-1607-4f5d-be59-87cc146fb611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.29154967e-07 3.00148920e-07 4.71142872e-07 6.42136825e-07\n",
      " 8.13130777e-07 9.84124730e-07 1.15511868e-06 1.32611263e-06\n",
      " 1.49710659e-06 1.66810054e-06]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.647319544531539"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(alpha_values)\n",
    "-0.5742146245315389 -0.07310492\n",
    "# -0.28685476121797904 [9.651254005555555e-07, 31] # coarse aggregate\n",
    "# -0.5233298770915911 [1.5777402395e-06, 10000] # fine aggregate\n",
    "# -0.2900672144550507 [0.0001, 15000] # age\n",
    "# -2.571625288578917 [7.554e-05, 31] # blast furnace slag\n",
    "# -0.5249004344951762 [2.9e-05, 500] # water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "1bcaf8d7-09ca-4f24-b575-9785e7bcb059",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kingh\\AppData\\Local\\Temp\\ipykernel_32256\\1696712276.py:21: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  m = m - alpha*m_gradient\n",
      "C:\\Users\\kingh\\AppData\\Local\\Temp\\ipykernel_32256\\1696712276.py:19: RuntimeWarning: invalid value encountered in multiply\n",
      "  m_gradient = np.mean(-2*X*(y - (m*X + b)))\n",
      "C:\\Users\\kingh\\AppData\\Local\\Temp\\ipykernel_32256\\1696712276.py:20: RuntimeWarning: invalid value encountered in multiply\n",
      "  b_gradient = np.mean(-2*(y - (m*X + b)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m [[       nan]\n",
      " [       nan]\n",
      " [0.00383079]\n",
      " [       nan]\n",
      " [1.56875327]\n",
      " [       nan]\n",
      " [       nan]\n",
      " [0.13139059]]\n",
      "b [[        nan]\n",
      " [        nan]\n",
      " [32.65221781]\n",
      " [        nan]\n",
      " [22.675146  ]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [27.9536373 ]]\n",
      "Test VE: [        nan         nan -0.09774375         nan  0.25866472         nan\n",
      "         nan -0.29006721]\n",
      "Train VE: [        nan         nan -0.06695411         nan -0.00486347         nan\n",
      "         nan  0.072547  ]\n",
      "Test MSE: [         nan          nan 239.02184727          nan 161.4177514\n",
      "          nan          nan 280.89820414]\n",
      "Train MSE: [         nan          nan 295.63199279          nan 278.4278962\n",
      "          nan          nan 256.97897737]\n"
     ]
    }
   ],
   "source": [
    "### TRAIN + ANALYZE UNIVARIATE MODELS (RAW)\n",
    "\n",
    "# initialize parameters\n",
    "num_feats = len(X_train[0])              # number of features\n",
    "alpha = 0.0001                              # learning rate       \n",
    "max_iter = 15000                            # number of iterations\n",
    "\n",
    "# conduct gradient descent to obtain a univariate linear regression model for each feature\n",
    "m = np.zeros(num_feats)\n",
    "b = np.zeros(num_feats)\n",
    "for i in range(num_feats):\n",
    "    X_i = X_train[:, i].reshape((-1,1))\n",
    "    m_i, b_i = uni_gradient_descent(X_i, y_train, alpha, max_iter)\n",
    "    m[i] = m_i\n",
    "    b[i] = b_i\n",
    "\n",
    "# evaluate variances\n",
    "var_train = np.var(y_train)\n",
    "var_test = np.var(y_test)\n",
    "\n",
    "# model predictions\n",
    "m = m.reshape(-1,1)\n",
    "b = b.reshape(-1,1)\n",
    "y_train_pred = (X_train.transpose()*m + b).transpose()\n",
    "y_test_pred = (X_test.transpose()*m + b).transpose()\n",
    "\n",
    "# train + test MSE\n",
    "train_MSE = np.mean((y_train_pred - np.tile(y_train, (1, num_feats)))**2, axis=0)\n",
    "test_MSE  = np.mean((y_test_pred - np.tile(y_test, (1, num_feats)))**2, axis=0)\n",
    "\n",
    "# train + test VE\n",
    "train_VE = 1 - (train_MSE / var_train)\n",
    "test_VE  = 1 - (test_MSE / var_test)\n",
    "\n",
    "print(\"m\", m)\n",
    "print(\"b\", b)\n",
    "print(\"Test VE:\", test_VE)\n",
    "print(\"Train VE:\", train_VE)\n",
    "print(\"Test MSE:\", test_MSE)\n",
    "print(\"Train MSE:\", train_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfbfdff-1faa-4509-ab4e-12d9da8046a7",
   "metadata": {},
   "source": [
    "## MULTIVARIATE LINEAR MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e307aa33-1b92-4725-96f3-3257fe84211f",
   "metadata": {},
   "source": [
    "## NORMALIZED PREDICTORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d20da3a-d068-4291-8794-f475f6e385f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: [0.021544346900318832, 1000]\n",
      "Best Test VE: 0.36691753817427264\n",
      "Corresponding Train VE: 0.568385102525635\n",
      "m: [[ 35.03827856]\n",
      " [ 17.72233663]\n",
      " [  3.27489821]\n",
      " [-11.19919354]\n",
      " [ 19.21362897]\n",
      " [  2.22733177]\n",
      " [ -0.97891267]\n",
      " [ 26.83758829]]\n",
      "b: 14.735515963644607\n"
     ]
    }
   ],
   "source": [
    "### GRID SEARCH FOR OPTIMAL HYPERPARAMS (MULTIVARIATE, NORMALIZED)\n",
    "\n",
    "alpha_values = [0.021544346900318832, 0.021544346900318832]\n",
    "max_iter_values = np.logspace(3, 5, 10, dtype=int)\n",
    "best_m = None\n",
    "best_b = None\n",
    "best_VE_train = None\n",
    "best_params = None\n",
    "best_VE = -np.inf\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    for max_iter in max_iter_values:\n",
    "        \n",
    "        # conduct gradient descent to obtain a multivariate linear regression model using all features\n",
    "        m, b = multi_gradient_descent(X_train_norm, y_train, alpha, max_iter)\n",
    "        \n",
    "        # evaluate variances\n",
    "        var_train = np.var(y_train)\n",
    "        var_test = np.var(y_test)\n",
    "        \n",
    "        # model predictions\n",
    "        y_train_pred = X_train_norm@m + b\n",
    "        y_test_pred = X_test_norm@m + b\n",
    "        \n",
    "        # train + test MSE\n",
    "        train_MSE = mean_squared_error(y_train, y_train_pred)\n",
    "        test_MSE  = mean_squared_error(y_test, y_test_pred)\n",
    "        \n",
    "        # train + test VE\n",
    "        train_VE = 1 - (train_MSE / var_train)\n",
    "        test_VE  = 1 - (test_MSE / var_test)\n",
    "        if test_VE > best_VE:\n",
    "            best_VE = test_VE\n",
    "            best_params = [alpha, max_iter]\n",
    "            best_m = m\n",
    "            best_b = b\n",
    "            best_VE_train = train_VE\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Test VE:\", best_VE)\n",
    "print(\"Corresponding Train VE:\", best_VE_train)\n",
    "print(\"m:\", best_m)\n",
    "print(\"b:\", best_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "915f6241-9549-4d77-b9a5-5d36cddb9924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m: [[ 35.18964021]\n",
      " [ 17.84399068]\n",
      " [  3.38439654]\n",
      " [-11.41243485]\n",
      " [ 19.23354567]\n",
      " [  2.22605995]\n",
      " [ -1.03417359]\n",
      " [ 27.13892915]]\n",
      "b: 14.696279123791093\n",
      "Test VE: 0.3672723351659948\n",
      "Train VE: 0.5698961819760422\n",
      "Test MSE: 137.76961600809852\n",
      "Train MSE: 119.17330563402975\n"
     ]
    }
   ],
   "source": [
    "### TRAIN + ANALYZE MULTIVARIATE MODELS (NORMALIZED)\n",
    "\n",
    "# initialize parameters\n",
    "alpha = 0.022                              # learning rate       \n",
    "max_iter = 1000                            # number of iterations\n",
    "\n",
    "# conduct gradient descent to obtain a multivariate linear regression model using all features\n",
    "m, b = multi_gradient_descent(X_train_norm, y_train, alpha, max_iter)\n",
    "\n",
    "# evaluate variances\n",
    "var_train = np.var(y_train)\n",
    "var_test = np.var(y_test)\n",
    "\n",
    "# model predictions\n",
    "y_train_pred = X_train_norm@m + b\n",
    "y_test_pred = X_test_norm@m + b\n",
    "\n",
    "# train + test MSE\n",
    "train_MSE = mean_squared_error(y_train, y_train_pred)\n",
    "test_MSE  = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "# train + test VE\n",
    "train_VE = 1 - (train_MSE / var_train)\n",
    "test_VE  = 1 - (test_MSE / var_test)\n",
    "\n",
    "print(\"m:\", m)\n",
    "print(\"b:\", b)\n",
    "print(\"Test VE:\", test_VE)\n",
    "print(\"Train VE:\", train_VE)\n",
    "print(\"Test MSE:\", test_MSE)\n",
    "print(\"Train MSE:\", train_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8747d0-87cf-4315-b4fa-a0e7250bb990",
   "metadata": {},
   "source": [
    "## RAW PREDICTORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74021cf6-cbca-4bc2-8eeb-86e09e3f345a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: [3.59381366e-07, 4641]\n",
      "Best Test VE: 0.5087655555166335\n",
      "Corresponding Train VE: 0.5718579694046066\n",
      "m: [[ 0.08774793]\n",
      " [ 0.06561048]\n",
      " [ 0.03166866]\n",
      " [-0.03871413]\n",
      " [ 0.92716067]\n",
      " [ 0.00549369]\n",
      " [-0.00615953]\n",
      " [ 0.10566376]]\n",
      "b: 0.9967969828652573\n"
     ]
    }
   ],
   "source": [
    "### GRID SEARCH FOR OPTIMAL HYPERPARAMS (MULTIVARIATE, RAW)\n",
    "\n",
    "alpha_values = [3.59381366e-07, 3.59381366e-07]\n",
    "max_iter_values = np.logspace(3, 5, 10, dtype=int)\n",
    "best_m = None\n",
    "best_b = None\n",
    "best_VE_train = None\n",
    "best_params = None\n",
    "best_VE = -np.inf\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    for max_iter in max_iter_values:\n",
    "        \n",
    "        # conduct gradient descent to obtain a multivariate linear regression model using all features\n",
    "        m, b = multi_gradient_descent(X_train, y_train, alpha, max_iter)\n",
    "        \n",
    "        # evaluate variances\n",
    "        var_train = np.var(y_train)\n",
    "        var_test = np.var(y_test)\n",
    "        \n",
    "        # model predictions\n",
    "        y_train_pred = X_train@m + b\n",
    "        y_test_pred = X_test@m + b\n",
    "        \n",
    "        # train + test MSE\n",
    "        train_MSE = mean_squared_error(y_train, y_train_pred)\n",
    "        test_MSE  = mean_squared_error(y_test, y_test_pred)\n",
    "        \n",
    "        # train + test VE\n",
    "        train_VE = 1 - (train_MSE / var_train)\n",
    "        test_VE  = 1 - (test_MSE / var_test)\n",
    "        if test_VE > best_VE:\n",
    "            best_VE = test_VE\n",
    "            best_params = [alpha, max_iter]\n",
    "            best_m = m\n",
    "            best_b = b\n",
    "            best_VE_train = train_VE\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Test VE:\", best_VE)\n",
    "print(\"Corresponding Train VE:\", best_VE_train)\n",
    "print(\"m:\", best_m)\n",
    "print(\"b:\", best_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b0636e8-9ffe-4fd9-830a-61a005a0334a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m: [[ 0.08770124]\n",
      " [ 0.06513533]\n",
      " [ 0.03186483]\n",
      " [-0.03387814]\n",
      " [ 0.92776233]\n",
      " [ 0.00482891]\n",
      " [-0.00636438]\n",
      " [ 0.10498659]]\n",
      "b: 0.9968046353632825\n",
      "Test VE: 0.5091996724879673\n",
      "Train VE: 0.5699769001358561\n",
      "Test MSE: 106.86647102070529\n",
      "Train MSE: 119.15094022008408\n"
     ]
    }
   ],
   "source": [
    "### TRAIN + ANALYZE MULTIVARIATE MODELS (RAW)\n",
    "# initialize parameters\n",
    "alpha = 3.6e-07                              # learning rate       \n",
    "max_iter = 4500                            # number of iterations\n",
    "\n",
    "# conduct gradient descent to obtain a multivariate linear regression model using all features\n",
    "m, b = multi_gradient_descent(X_train, y_train, alpha, max_iter)\n",
    "\n",
    "# evaluate variances\n",
    "var_train = np.var(y_train)\n",
    "var_test = np.var(y_test)\n",
    "\n",
    "# model predictions\n",
    "y_train_pred = X_train@m + b\n",
    "y_test_pred = X_test@m + b\n",
    "\n",
    "# train + test MSE\n",
    "train_MSE = mean_squared_error(y_train, y_train_pred)\n",
    "test_MSE  = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "# train + test VE\n",
    "train_VE = 1 - (train_MSE / var_train)\n",
    "test_VE  = 1 - (test_MSE / var_test)\n",
    "\n",
    "print(\"m:\", m)\n",
    "print(\"b:\", b)\n",
    "print(\"Test VE:\", test_VE)\n",
    "print(\"Train VE:\", train_VE)\n",
    "print(\"Test MSE:\", test_MSE)\n",
    "print(\"Train MSE:\", train_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44892e7-fbe8-44c2-86fd-c3d6bffead49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
