{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "022a1013-595b-40d9-a86c-711b397d3126",
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTS\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7bc2478-cec1-4e83-81da-80b1f7370abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTIONS\n",
    "\n",
    "## Min-Max Normalization\n",
    "# X: array to be normalized\n",
    "def normalize(X):\n",
    "    X_norm = (X-np.min(X, axis=0))/(np.max(X, axis=0) - np.min(X, axis=0))\n",
    "    return X_norm\n",
    "\n",
    "## Univariate Gradient Descent\n",
    "# X: feature vector\n",
    "# y: target vector\n",
    "# alpha: step size\n",
    "# max_iter: number of iterations \n",
    "def uni_gradient_descent(X, y, alpha, max_iter):\n",
    "    m = 1\n",
    "    b = 1\n",
    "    n = len(X)\n",
    "    for i in range(max_iter):\n",
    "        m_gradient = np.mean(-2*X*(y - (m*X + b)))\n",
    "        b_gradient = np.mean(-2*(y - (m*X + b)))\n",
    "        m = m - alpha*m_gradient\n",
    "        b = b - alpha*b_gradient\n",
    "    return m, b\n",
    "\n",
    "### Multivariate Gradient Descent\n",
    "# X: feature array \n",
    "# y: target vector\n",
    "# alpha: step size\n",
    "# max_iter: number of iterations \n",
    "def multi_gradient_descent(X, y, alpha, max_iter):\n",
    "    if (X.ndim == 1):\n",
    "        X = X.reshape(1, -1)\n",
    "    n, num_feats = X.shape\n",
    "    m = np.ones((num_feats,1))\n",
    "    b = 1\n",
    "    for i in range(max_iter):\n",
    "        m_gradient = np.sum(-2*(y - (X@m + b))*X, axis=0).reshape(1, -1).transpose()\n",
    "        b_gradient = np.sum(-2*(y - (X@m + b)))\n",
    "        m = m - alpha*m_gradient/n\n",
    "        b = b - alpha*b_gradient/n\n",
    "    return m, b\n",
    "\n",
    "### Multivariate Gradient Descent w/ MSE\n",
    "# X: feature array \n",
    "# y: target vector\n",
    "# alpha: step size\n",
    "# max_iter: number of iterations \n",
    "def multi_gradient_descent_MSE(X, y, alpha, max_iter):\n",
    "    if (X.ndim == 1):\n",
    "        X = X.reshape(1, -1)\n",
    "    n, num_feats = X.shape\n",
    "    m = np.ones((num_feats,1))\n",
    "    MSE = []\n",
    "    b = 1\n",
    "    for i in range(max_iter):\n",
    "        m_gradient = np.sum(-2*(y - (X@m + b))*X, axis=0).reshape(1, -1).transpose()\n",
    "        b_gradient = np.sum(-2*(y - (X@m + b)))\n",
    "        m = m - alpha*m_gradient/n\n",
    "        b = b - alpha*b_gradient/n\n",
    "        MSE.append(mean_squared_error(y, X@m + b))\n",
    "    return m, b, MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26d6f423-b7e2-4368-8f05-43b1323ee128",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREPROCESSING I\n",
    "\n",
    "# read data set\n",
    "data = pd.read_csv(r\"C:\\Users\\kingh\\Downloads\\Concrete_Data.csv\")\n",
    "\n",
    "# extract feature variables\n",
    "X = data.iloc[:,:8].to_numpy()\n",
    "X_test = X[501:631, :]\n",
    "X_train = np.vstack((X[0:501, :], X[631:, :]))\n",
    "\n",
    "# extract target variable\n",
    "y = data.iloc[:, -1].to_numpy().reshape((-1,1))\n",
    "y_test = y[501:631]\n",
    "y_train = np.vstack((y[0:501], y[631:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29ba3542-13b9-49b5-9b2b-821598a03399",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREPROCESSING II\n",
    "\n",
    "# create normalized X\n",
    "X_train_norm = normalize(X_train)\n",
    "X_test_norm = normalize(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c4d8fe-0870-45f7-b019-a493dc8da491",
   "metadata": {},
   "source": [
    "# UNIVARIATE LINEAR MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32b8ded-1bf9-4797-90ca-caa40c21a6d9",
   "metadata": {},
   "source": [
    "## NORMALIZED PREDICTORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e25fae3e-30b8-4cec-8d3b-886d639059ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Average Train VE: -0.053353590339357534\n",
      "Best Parameters: [0.001, 1000]\n",
      "Best Test VE: [ 0.00072418 -0.13550305  0.05857924 -0.29597445  0.09836463 -0.41566435\n",
      " -0.26336916 -0.05752401]\n",
      "Corresponding Train VE: [ 0.14229641 -0.03974582 -0.13704715 -0.12694088 -0.01299616 -0.10974377\n",
      " -0.10767421 -0.03497713]\n",
      "m: [[14.82261933]\n",
      " [ 8.86393444]\n",
      " [ 6.28057738]\n",
      " [11.55899224]\n",
      " [ 8.89856738]\n",
      " [12.38175904]\n",
      " [11.4646642 ]\n",
      " [ 6.49420175]]\n",
      "b: [[27.96240104]\n",
      " [30.53586387]\n",
      " [30.68372789]\n",
      " [28.05459217]\n",
      " [30.79594057]\n",
      " [27.48930174]\n",
      " [28.31462125]\n",
      " [31.56115065]]\n"
     ]
    }
   ],
   "source": [
    "### GRID SEARCH FOR OPTIMAL HYPERPARAMS (UNIVARIATE, NORMALIZED)\n",
    "\n",
    "alpha_values = np.logspace(-8, -3, 10)\n",
    "max_iter_values = np.logspace(1, 3, 5, dtype=int)\n",
    "best_m = None\n",
    "best_b = None\n",
    "best_VE_train = None\n",
    "best_params = None\n",
    "best_VE = None\n",
    "best_avg_VE = -np.inf\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    for max_iter in max_iter_values:\n",
    "        \n",
    "        # initialize parameters\n",
    "        num_feats = len(X_train_norm[0])          # number of features\n",
    "        \n",
    "        # conduct gradient descent to obtain a univariate linear regression model for each feature\n",
    "        m = np.zeros(num_feats)\n",
    "        b = np.zeros(num_feats)\n",
    "        for i in range(num_feats):\n",
    "            X_i = X_train_norm[:, i].reshape((-1,1))\n",
    "            m_i, b_i = uni_gradient_descent(X_i, y_train, alpha, max_iter)\n",
    "            m[i] = m_i\n",
    "            b[i] = b_i\n",
    "        \n",
    "        # evaluate variances\n",
    "        var_train = np.var(y_train)\n",
    "        var_test = np.var(y_test)\n",
    "        \n",
    "        # model predictions\n",
    "        m = m.reshape(-1,1)\n",
    "        b = b.reshape(-1,1)\n",
    "        y_train_pred = (X_train_norm.transpose()*m + b).transpose()\n",
    "        y_test_pred = (X_test_norm.transpose()*m + b).transpose()\n",
    "        \n",
    "        # train + test MSE\n",
    "        train_MSE = np.mean((y_train_pred - np.tile(y_train, (1, num_feats)))**2, axis=0)\n",
    "        test_MSE  = np.mean((y_test_pred - np.tile(y_test, (1, num_feats)))**2, axis=0)\n",
    "        \n",
    "        # train + test VE\n",
    "        train_VE = 1 - (train_MSE / var_train)\n",
    "        test_VE  = 1 - (test_MSE / var_test)\n",
    "        \n",
    "        # update best model based on average train VE\n",
    "        if np.mean(train_VE) > best_avg_VE:\n",
    "            best_avg_VE = np.mean(train_VE)\n",
    "            best_VE = test_VE\n",
    "            best_params = [alpha, max_iter]\n",
    "            best_m = m\n",
    "            best_b = b\n",
    "            best_VE_train = train_VE\n",
    "\n",
    "print(\"Best Average Train VE:\", best_avg_VE)\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Test VE:\", best_VE)\n",
    "print(\"Corresponding Train VE:\", best_VE_train)\n",
    "print(\"m:\", best_m)\n",
    "print(\"b:\", best_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7c40111-c9bd-4a61-83aa-5fadd5b944f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m: [[ 35.383]\n",
      " [  9.53 ]\n",
      " [-10.67 ]\n",
      " [-25.778]\n",
      " [ 28.434]\n",
      " [ -9.442]\n",
      " [-12.209]\n",
      " [ 34.577]]\n",
      "b: [[22.741]\n",
      " [34.595]\n",
      " [40.039]\n",
      " [49.107]\n",
      " [30.907]\n",
      " [41.705]\n",
      " [42.429]\n",
      " [32.693]]\n",
      "Test VE: [[-0.2642]\n",
      " [-0.3805]\n",
      " [-0.8215]\n",
      " [-0.1709]\n",
      " [ 0.1017]\n",
      " [-0.256 ]\n",
      " [-0.3409]\n",
      " [-0.3583]]\n",
      "Train VE: [[0.2655]\n",
      " [0.0248]\n",
      " [0.0423]\n",
      " [0.0759]\n",
      " [0.1005]\n",
      " [0.0164]\n",
      " [0.0227]\n",
      " [0.1223]]\n",
      "Test MSE: [[275.26]\n",
      " [300.58]\n",
      " [396.61]\n",
      " [254.96]\n",
      " [195.59]\n",
      " [273.48]\n",
      " [291.97]\n",
      " [295.75]]\n",
      "Train MSE: [[203.53]\n",
      " [270.2 ]\n",
      " [265.35]\n",
      " [256.05]\n",
      " [249.24]\n",
      " [272.55]\n",
      " [270.79]\n",
      " [243.19]]\n"
     ]
    }
   ],
   "source": [
    "### TRAIN + ANALYZE UNIVARIATE MODELS (NORMALIZED)\n",
    "\n",
    "# initialize parameters\n",
    "num_feats = len(X_train_norm[0])          # number of features\n",
    "alpha = 0.4756                              # learning rate       \n",
    "max_iter = 1000                            # number of iterations\n",
    "\n",
    "# conduct gradient descent to obtain a univariate linear regression model for each feature\n",
    "m = np.zeros(num_feats)\n",
    "b = np.zeros(num_feats)\n",
    "for i in range(num_feats):\n",
    "    X_i = X_train_norm[:, i].reshape((-1,1))\n",
    "    m_i, b_i = uni_gradient_descent(X_i, y_train, alpha, max_iter)\n",
    "    m[i] = m_i\n",
    "    b[i] = b_i\n",
    "\n",
    "# evaluate variances\n",
    "var_train = np.var(y_train)\n",
    "var_test = np.var(y_test)\n",
    "\n",
    "# model predictions\n",
    "m = m.reshape(-1,1)\n",
    "b = b.reshape(-1,1)\n",
    "y_train_pred = (X_train_norm.transpose()*m + b).transpose()\n",
    "y_test_pred = (X_test_norm.transpose()*m + b).transpose()\n",
    "\n",
    "# train + test MSE\n",
    "train_MSE = np.mean((y_train_pred - np.tile(y_train, (1, num_feats)))**2, axis=0)\n",
    "test_MSE  = np.mean((y_test_pred - np.tile(y_test, (1, num_feats)))**2, axis=0)\n",
    "\n",
    "# train + test VE\n",
    "train_VE = 1 - (train_MSE / var_train)\n",
    "test_VE  = 1 - (test_MSE / var_test)\n",
    "\n",
    "# rounding for ease of interpretation + postprocessing\n",
    "m_uvnorm = np.round_(m, decimals=3)\n",
    "b_uvnorm = np.round_(b, decimals=3)\n",
    "train_MSE_uvnorm = np.round_(train_MSE, decimals=2).reshape(-1,1)\n",
    "test_MSE_uvnorm  = np.round_(test_MSE, decimals=2).reshape(-1,1)\n",
    "train_VE_uvnorm = np.round_(train_VE, decimals=4).reshape(-1,1)\n",
    "test_VE_uvnorm  = np.round_(test_VE, decimals=4).reshape(-1,1)\n",
    "\n",
    "print(\"m:\", m_uvnorm)\n",
    "print(\"b:\", b_uvnorm)\n",
    "print(\"Test VE:\", test_VE_uvnorm)\n",
    "print(\"Train VE:\", train_VE_uvnorm)\n",
    "print(\"Test MSE:\", test_MSE_uvnorm)\n",
    "print(\"Train MSE:\", train_MSE_uvnorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbce214f-ddb6-484c-9d16-b608b846f575",
   "metadata": {},
   "source": [
    "## RAW PREDICTORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bbd804e-7b39-468f-b7b3-23898055da03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kingh\\miniconda3\\envs\\cse2107\\lib\\site-packages\\numpy\\core\\_methods.py:180: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "C:\\Users\\kingh\\AppData\\Local\\Temp\\ipykernel_24956\\1397851816.py:21: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  m = m - alpha*m_gradient\n",
      "C:\\Users\\kingh\\AppData\\Local\\Temp\\ipykernel_24956\\3475226586.py:38: RuntimeWarning: overflow encountered in square\n",
      "  train_MSE = np.mean((y_train_pred - np.tile(y_train, (1, num_feats)))**2, axis=0)\n",
      "C:\\Users\\kingh\\AppData\\Local\\Temp\\ipykernel_24956\\3475226586.py:39: RuntimeWarning: overflow encountered in square\n",
      "  test_MSE  = np.mean((y_test_pred - np.tile(y_test, (1, num_feats)))**2, axis=0)\n",
      "C:\\Users\\kingh\\AppData\\Local\\Temp\\ipykernel_24956\\1397851816.py:19: RuntimeWarning: invalid value encountered in multiply\n",
      "  m_gradient = np.mean(-2*X*(y - (m*X + b)))\n",
      "C:\\Users\\kingh\\AppData\\Local\\Temp\\ipykernel_24956\\1397851816.py:20: RuntimeWarning: invalid value encountered in multiply\n",
      "  b_gradient = np.mean(-2*(y - (m*X + b)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: [7.742636826811278e-05, 1000]\n",
      "Best Test VE: [        nan -2.93176516 -1.1609454          nan -1.43520318         nan\n",
      "         nan -2.33964926]\n",
      "Corresponding Train VE: [        nan -1.86234298 -2.52907085         nan -1.1015818          nan\n",
      "         nan -1.76792931]\n",
      "m: [[       nan]\n",
      " [0.21408177]\n",
      " [0.22390229]\n",
      " [       nan]\n",
      " [3.19931931]\n",
      " [       nan]\n",
      " [       nan]\n",
      " [0.31932661]]\n",
      "b: [[       nan]\n",
      " [3.73393931]\n",
      " [4.20967895]\n",
      " [       nan]\n",
      " [3.10099419]\n",
      " [       nan]\n",
      " [       nan]\n",
      " [3.97308755]]\n"
     ]
    }
   ],
   "source": [
    "### GRID SEARCH FOR OPTIMAL HYPERPARAMS (UNIVARIATE, RAW)\n",
    "\n",
    "feature_to_test = 1\n",
    "alpha_values = np.logspace(-8, -3, 10)\n",
    "max_iter_values = np.logspace(1, 3, 5, dtype=int)\n",
    "best_m = None\n",
    "best_b = None\n",
    "best_VE_train = None\n",
    "best_params = None\n",
    "best_VE = None\n",
    "best_avg_VE = -np.inf\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    for max_iter in max_iter_values:\n",
    "        # initialize parameters\n",
    "        num_feats = len(X_train[0])          # number of features\n",
    "        \n",
    "        # conduct gradient descent to obtain a univariate linear regression model for each feature\n",
    "        m = np.zeros(num_feats)\n",
    "        b = np.zeros(num_feats)\n",
    "        for i in range(num_feats):\n",
    "            X_i = X_train[:, i].reshape((-1,1))\n",
    "            m_i, b_i = uni_gradient_descent(X_i, y_train, alpha, max_iter)\n",
    "            m[i] = m_i\n",
    "            b[i] = b_i\n",
    "        \n",
    "        # evaluate variances\n",
    "        var_train = np.var(y_train)\n",
    "        var_test = np.var(y_test)\n",
    "        \n",
    "        # model predictions\n",
    "        m = m.reshape(-1,1)\n",
    "        b = b.reshape(-1,1)\n",
    "        y_train_pred = (X_train.transpose()*m + b).transpose()\n",
    "        y_test_pred = (X_test.transpose()*m + b).transpose()\n",
    "        \n",
    "        # train + test MSE\n",
    "        train_MSE = np.mean((y_train_pred - np.tile(y_train, (1, num_feats)))**2, axis=0)\n",
    "        test_MSE  = np.mean((y_test_pred - np.tile(y_test, (1, num_feats)))**2, axis=0)\n",
    "                \n",
    "        # train + test VE\n",
    "        train_VE = 1 - (train_MSE / var_train)\n",
    "        test_VE  = 1 - (test_MSE / var_test)\n",
    "\n",
    "        # update best model based on train VE\n",
    "        if train_VE[feature_to_test] > best_avg_VE:\n",
    "            best_avg_VE = train_VE[feature_to_test]\n",
    "            best_VE = test_VE\n",
    "            best_params = [alpha, max_iter]\n",
    "            best_m = m\n",
    "            best_b = b\n",
    "            best_VE_train = train_VE\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Test VE:\", best_VE)\n",
    "print(\"Corresponding Train VE:\", best_VE_train)\n",
    "print(\"m:\", best_m)\n",
    "print(\"b:\", best_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bcaf8d7-09ca-4f24-b575-9785e7bcb059",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m [[ 1.15280794e-01]\n",
      " [ 2.30190712e-01]\n",
      " [ 2.51390540e-01]\n",
      " [ 1.92988180e-01]\n",
      " [ 1.13224865e+00]\n",
      " [-1.75759647e+42]\n",
      " [-1.69009047e+35]\n",
      " [ 3.50960399e-01]]\n",
      "b [[ 9.98416977e-01]\n",
      " [ 1.00825782e+00]\n",
      " [ 1.00926530e+00]\n",
      " [ 9.96362741e-01]\n",
      " [ 1.01991034e+00]\n",
      " [-1.79417194e+39]\n",
      " [-2.16439507e+32]\n",
      " [ 1.00894937e+00]]\n",
      "Test VE: [ 3.52251366e-02 -3.47746180e+00 -1.63391312e+00 -5.25086838e-01\n",
      " -2.63285164e+00 -1.34187267e+88 -8.10584497e+73 -2.96999753e+00]\n",
      "Train VE: [ 1.61971603e-01 -2.21041149e+00 -3.00916814e+00 -2.13319094e-01\n",
      " -2.77251412e+00 -1.06315387e+88 -6.21594787e+73 -2.18117489e+00]\n",
      "Test MSE: [2.10069308e+02 9.74918954e+02 5.73506138e+02 3.32071189e+02\n",
      " 7.91014212e+02 2.92178283e+90 1.76496021e+76 8.64424089e+02]\n",
      "Train MSE: [2.32201180e+02 8.89541859e+02 1.11086161e+03 3.36186849e+02\n",
      " 1.04528944e+03 2.94579020e+90 1.72231685e+76 8.81440971e+02]\n"
     ]
    }
   ],
   "source": [
    "### TRAIN + ANALYZE UNIVARIATE MODELS (RAW)\n",
    "\n",
    "# initialize parameters\n",
    "num_feats = len(X_train[0])              # number of features\n",
    "alpha = 1.053e-5                              # learning rate       \n",
    "max_iter = 33                            # number of iterations\n",
    "\n",
    "# conduct gradient descent to obtain a univariate linear regression model for each feature\n",
    "m = np.zeros(num_feats)\n",
    "b = np.zeros(num_feats)\n",
    "for i in range(num_feats):\n",
    "    X_i = X_train[:, i].reshape((-1,1))\n",
    "    m_i, b_i = uni_gradient_descent(X_i, y_train, alpha, max_iter)\n",
    "    m[i] = m_i\n",
    "    b[i] = b_i\n",
    "\n",
    "# evaluate variances\n",
    "var_train = np.var(y_train)\n",
    "var_test = np.var(y_test)\n",
    "\n",
    "# model predictions\n",
    "m = m.reshape(-1,1)\n",
    "b = b.reshape(-1,1)\n",
    "y_train_pred = (X_train.transpose()*m + b).transpose()\n",
    "y_test_pred = (X_test.transpose()*m + b).transpose()\n",
    "\n",
    "# train + test MSE\n",
    "train_MSE = np.mean((y_train_pred - np.tile(y_train, (1, num_feats)))**2, axis=0)\n",
    "test_MSE  = np.mean((y_test_pred - np.tile(y_test, (1, num_feats)))**2, axis=0)\n",
    "\n",
    "# train + test VE\n",
    "train_VE = 1 - (train_MSE / var_train)\n",
    "test_VE  = 1 - (test_MSE / var_test)\n",
    "\n",
    "print(\"m\", m)\n",
    "print(\"b\", b)\n",
    "print(\"Test VE:\", test_VE)\n",
    "print(\"Train VE:\", train_VE)\n",
    "print(\"Test MSE:\", test_MSE)\n",
    "print(\"Train MSE:\", train_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5ebe574-1607-4f5d-be59-87cc146fb611",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEST HYPERPARAMETER PAIRS FOR UNIVARIATE, RAW MODELS\n",
    "# [1.053e-5, 33] # cement\n",
    "# [7.554e-05, 31] # blast furnace slag\n",
    "# [1.3445e-4, 10000] # fly ash\n",
    "# [2.9e-05, 500] # water\n",
    "# [0.001, 10000] # superplasticizer\n",
    "# [9.651254005555555e-07, 31] # coarse aggregate\n",
    "# [1.5777402395e-06, 10000] # fine aggregate\n",
    "# [0.0001, 15000] # age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9caf32c4-41e0-49e0-83f3-37a719a11fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAIN UNIVARIATE MODELS (RAW) USING BEST HYPERPARAM PAIRS FOR EACH FEATURE + STORE RESULTS FOR POSTPROCESSING\n",
    "\n",
    "best_hyperparams = np.array([[1.053e-5, 33], [7.554e-5, 31], [1.3445e-4, 10000], [2.9e-5, 500], [0.001, 10000], [9.6513e-7, 31], [1.5777e-6, 10000], [0.0001, 15000]])\n",
    "m_uvraw = []\n",
    "b_uvraw = []\n",
    "train_MSE_uvraw = []\n",
    "train_VE_uvraw = []\n",
    "test_MSE_uvraw = []\n",
    "test_VE_uvraw = []\n",
    "\n",
    "i = 0\n",
    "for params in best_hyperparams:\n",
    "    # initialize parameters\n",
    "    alpha = params[0]                              # learning rate       \n",
    "    max_iter = int(params[1])                            # number of iterations\n",
    "    \n",
    "    # conduct gradient descent to obtain a univariate linear regression model for each feature\n",
    "    X_i = X_train[:, i].reshape((-1,1))\n",
    "    m, b = uni_gradient_descent(X_i, y_train, alpha, max_iter)\n",
    "    \n",
    "    # evaluate variances\n",
    "    var_train = np.var(y_train)\n",
    "    var_test = np.var(y_test)\n",
    "    \n",
    "    # model predictions\n",
    "    y_train_pred = (X_i*m + b)\n",
    "    y_test_pred = (X_test[:, i].reshape((-1,1))*m + b)\n",
    "    \n",
    "    # train + test MSE\n",
    "    train_MSE = np.mean((y_train_pred - y_train)**2)\n",
    "    test_MSE  = np.mean((y_test_pred - y_test)**2)\n",
    "    \n",
    "    # train + test VE\n",
    "    train_VE = 1 - (train_MSE / var_train)\n",
    "    test_VE  = 1 - (test_MSE / var_test)\n",
    "\n",
    "    # store parameters + results    \n",
    "    m_uvraw.append(m)\n",
    "    b_uvraw.append(b)\n",
    "    train_MSE_uvraw.append(train_MSE)\n",
    "    train_VE_uvraw.append(train_VE)\n",
    "    test_MSE_uvraw.append(test_MSE)\n",
    "    test_VE_uvraw.append(test_VE)\n",
    "\n",
    "    i+=1\n",
    "\n",
    "# rounding for ease of interpretation + post-processing\n",
    "m_uvraw = np.round_(m_uvraw, decimals=3)\n",
    "b_uvraw = np.round_(b_uvraw, decimals=3)\n",
    "train_MSE_uvraw = np.round_(train_MSE_uvraw, decimals=2)\n",
    "test_MSE_uvraw  = np.round_(test_MSE_uvraw, decimals=2)\n",
    "train_VE_uvraw = np.round_(train_VE_uvraw, decimals=4)\n",
    "test_VE_uvraw  = np.round_(test_VE_uvraw, decimals=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfbfdff-1faa-4509-ab4e-12d9da8046a7",
   "metadata": {},
   "source": [
    "## MULTIVARIATE LINEAR MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e307aa33-1b92-4725-96f3-3257fe84211f",
   "metadata": {},
   "source": [
    "## NORMALIZED PREDICTORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d20da3a-d068-4291-8794-f475f6e385f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: [0.001, 1000]\n",
      "Best Test VE: 0.14273968177036866\n",
      "Corresponding Train VE: 0.14273968177036866\n",
      "m: [[11.21455603]\n",
      " [ 6.53142702]\n",
      " [ 3.47526697]\n",
      " [ 7.04261211]\n",
      " [ 6.16033881]\n",
      " [ 7.90278336]\n",
      " [ 7.14719488]\n",
      " [ 4.78152562]]\n",
      "b: 16.950570827412072\n"
     ]
    }
   ],
   "source": [
    "### GRID SEARCH FOR OPTIMAL HYPERPARAMS (MULTIVARIATE, NORMALIZED)\n",
    "\n",
    "alpha_values = np.logspace(-8, -3, 10)\n",
    "max_iter_values = np.logspace(1, 3, 5, dtype=int)\n",
    "best_m = None\n",
    "best_b = None\n",
    "best_VE_train = None\n",
    "best_params = None\n",
    "best_VE = -np.inf\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    for max_iter in max_iter_values:\n",
    "        \n",
    "        # conduct gradient descent to obtain a multivariate linear regression model using all features\n",
    "        m, b = multi_gradient_descent(X_train_norm, y_train, alpha, max_iter)\n",
    "        \n",
    "        # evaluate variances\n",
    "        var_train = np.var(y_train)\n",
    "        var_test = np.var(y_test)\n",
    "        \n",
    "        # model predictions\n",
    "        y_train_pred = X_train_norm@m + b\n",
    "        y_test_pred = X_test_norm@m + b\n",
    "        \n",
    "        # train + test MSE\n",
    "        train_MSE = mean_squared_error(y_train, y_train_pred)\n",
    "        test_MSE  = mean_squared_error(y_test, y_test_pred)\n",
    "        \n",
    "        # train + test VE\n",
    "        train_VE = 1 - (train_MSE / var_train)\n",
    "        test_VE  = 1 - (test_MSE / var_test)\n",
    "        # update best model based on train VE\n",
    "        if train_VE > best_VE:\n",
    "            best_VE = train_VE\n",
    "            best_params = [alpha, max_iter]\n",
    "            best_m = m\n",
    "            best_b = b\n",
    "            best_VE_train = train_VE\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Test VE:\", best_VE)\n",
    "print(\"Corresponding Train VE:\", best_VE_train)\n",
    "print(\"m:\", best_m)\n",
    "print(\"b:\", best_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "915f6241-9549-4d77-b9a5-5d36cddb9924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m: [[ 49.22 ]\n",
      " [ 30.668]\n",
      " [ 15.439]\n",
      " [-26.41 ]\n",
      " [  3.333]\n",
      " [  2.96 ]\n",
      " [  3.889]\n",
      " [ 42.894]]\n",
      "b: 8.295\n",
      "Test VE: 0.341\n",
      "Train VE: 0.6219\n",
      "Test MSE: 143.49\n",
      "Train MSE: 104.76\n"
     ]
    }
   ],
   "source": [
    "### TRAIN + ANALYZE MULTIVARIATE MODELS (NORMALIZED)\n",
    "\n",
    "# initialize parameters\n",
    "alpha = 0.0215                              # learning rate       \n",
    "max_iter = 10000                            # number of iterations\n",
    "\n",
    "# conduct gradient descent to obtain a multivariate linear regression model using all features\n",
    "m, b = multi_gradient_descent(X_train_norm, y_train, alpha, max_iter)\n",
    "\n",
    "# evaluate variances\n",
    "var_train = np.var(y_train)\n",
    "var_test = np.var(y_test)\n",
    "\n",
    "# model predictions\n",
    "y_train_pred = X_train_norm@m + b\n",
    "y_test_pred = X_test_norm@m + b\n",
    "m = np.round_(m, decimals=3)\n",
    "b = np.round_(b, decimals=3)\n",
    "\n",
    "# train + test MSE\n",
    "train_MSE = mean_squared_error(y_train, y_train_pred)\n",
    "test_MSE  = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "# train + test VE\n",
    "train_VE = 1 - (train_MSE / var_train)\n",
    "test_VE  = 1 - (test_MSE / var_test)\n",
    "\n",
    "# rounding for ease of interpretation + post-processing\n",
    "m_mvnorm = np.round_(m, decimals=3)\n",
    "b_mvnorm = np.round_(b, decimals=3)\n",
    "train_MSE_mvnorm = np.round_(train_MSE, decimals=2)\n",
    "test_MSE_mvnorm  = np.round_(test_MSE, decimals=2)\n",
    "train_VE_mvnorm = np.round_(train_VE, decimals=4)\n",
    "test_VE_mvnorm  = np.round_(test_VE, decimals=4)\n",
    "\n",
    "print(\"m:\", m_mvnorm)\n",
    "print(\"b:\", b_mvnorm)\n",
    "print(\"Test VE:\", test_VE_mvnorm)\n",
    "print(\"Train VE:\", train_VE_mvnorm)\n",
    "print(\"Test MSE:\", test_MSE_mvnorm)\n",
    "print(\"Train MSE:\", train_MSE_mvnorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8747d0-87cf-4315-b4fa-a0e7250bb990",
   "metadata": {},
   "source": [
    "## RAW PREDICTORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74021cf6-cbca-4bc2-8eeb-86e09e3f345a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: [4.6415888336127725e-07, 100]\n",
      "Best Test VE: -15.878863532759095\n",
      "Corresponding Train VE: -15.878863532759095\n",
      "m: [[ 0.43596522]\n",
      " [ 0.58656564]\n",
      " [ 0.8877029 ]\n",
      " [ 0.63678096]\n",
      " [ 0.98156711]\n",
      " [-0.30173424]\n",
      " [-0.04890931]\n",
      " [ 0.67920203]]\n",
      "b: 0.9983026320373659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kingh\\miniconda3\\envs\\cse2107\\lib\\site-packages\\sklearn\\metrics\\_regression.py:478: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n",
      "C:\\Users\\kingh\\miniconda3\\envs\\cse2107\\lib\\site-packages\\sklearn\\metrics\\_regression.py:478: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n"
     ]
    }
   ],
   "source": [
    "### GRID SEARCH FOR OPTIMAL HYPERPARAMS (MULTIVARIATE, RAW)\n",
    "\n",
    "alpha_values = np.logspace(-8, -5, 10)\n",
    "max_iter_values = np.logspace(1, 2, 5, dtype=int)\n",
    "best_m = None\n",
    "best_b = None\n",
    "best_VE_train = None\n",
    "best_params = None\n",
    "best_VE = -np.inf\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    for max_iter in max_iter_values:\n",
    "        \n",
    "        # conduct gradient descent to obtain a multivariate linear regression model using all features\n",
    "        m, b = multi_gradient_descent(X_train, y_train, alpha, max_iter)\n",
    "        \n",
    "        # evaluate variances\n",
    "        var_train = np.var(y_train)\n",
    "        var_test = np.var(y_test)\n",
    "        \n",
    "        # model predictions\n",
    "        y_train_pred = X_train@m + b\n",
    "        y_test_pred = X_test@m + b\n",
    "        \n",
    "        # train + test MSE\n",
    "        train_MSE = mean_squared_error(y_train, y_train_pred)\n",
    "        test_MSE  = mean_squared_error(y_test, y_test_pred)\n",
    "        \n",
    "        # train + test VE\n",
    "        train_VE = 1 - (train_MSE / var_train)\n",
    "        test_VE  = 1 - (test_MSE / var_test)\n",
    "        # update best model based on train VE\n",
    "        if train_VE > best_VE:\n",
    "            best_VE = train_VE\n",
    "            best_params = [alpha, max_iter]\n",
    "            best_m = m\n",
    "            best_b = b\n",
    "            best_VE_train = train_VE\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Test VE:\", best_VE)\n",
    "print(\"Corresponding Train VE:\", best_VE_train)\n",
    "print(\"m:\", best_m)\n",
    "print(\"b:\", best_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b0636e8-9ffe-4fd9-830a-61a005a0334a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m: [[ 0.091]\n",
      " [ 0.075]\n",
      " [ 0.034]\n",
      " [-0.112]\n",
      " [ 0.892]\n",
      " [ 0.015]\n",
      " [-0.003]\n",
      " [ 0.116]]\n",
      "b: 0.997\n",
      "Test VE: 0.4944\n",
      "Train VE: 0.5892\n",
      "Test MSE: 110.09\n",
      "Train MSE: 113.82\n"
     ]
    }
   ],
   "source": [
    "### TRAIN + ANALYZE MULTIVARIATE MODELS (RAW)\n",
    "# initialize parameters\n",
    "alpha = 3.594e-07                              # learning rate       \n",
    "max_iter = 10000                            # number of iterations\n",
    "\n",
    "# conduct gradient descent to obtain a multivariate linear regression model using all features\n",
    "m, b = multi_gradient_descent(X_train, y_train, alpha, max_iter)\n",
    "\n",
    "# evaluate variances\n",
    "var_train = np.var(y_train)\n",
    "var_test = np.var(y_test)\n",
    "\n",
    "# model predictions\n",
    "y_train_pred = X_train@m + b\n",
    "y_test_pred = X_test@m + b\n",
    "\n",
    "# train + test MSE\n",
    "train_MSE = mean_squared_error(y_train, y_train_pred)\n",
    "test_MSE  = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "# train + test VE\n",
    "train_VE = 1 - (train_MSE / var_train)\n",
    "test_VE  = 1 - (test_MSE / var_test)\n",
    "\n",
    "# rounding for ease of interpretation\n",
    "m_mvraw = np.round_(m, decimals=3)\n",
    "b_mvraw = np.round_(b, decimals=3)\n",
    "train_MSE_mvraw = np.round_(train_MSE, decimals=2)\n",
    "test_MSE_mvraw  = np.round_(test_MSE, decimals=2)\n",
    "train_VE_mvraw = np.round_(train_VE, decimals=4)\n",
    "test_VE_mvraw  = np.round_(test_VE, decimals=4)\n",
    "\n",
    "print(\"m:\", m_mvraw)\n",
    "print(\"b:\", b_mvraw)\n",
    "print(\"Test VE:\", test_VE_mvraw)\n",
    "print(\"Train VE:\", train_VE_mvraw)\n",
    "print(\"Test MSE:\", test_MSE_mvraw)\n",
    "print(\"Train MSE:\", train_MSE_mvraw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8183ff4c-06bf-4654-b04a-e334a051e6c4",
   "metadata": {},
   "source": [
    "# LOSS OVER ITERATIONS PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69cb0bae-0740-4c5a-af21-dea52d18fbee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjoAAAHFCAYAAAD7ZFORAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAABPP0lEQVR4nO3deVhU9f4H8PeZYWbYh01AFAQ1F0RAERXT3C3Xa4uZt1RS66pUmnXb9JZtl+qWaYX6s3K5tmjd1LylJuWeGyLmgrsgmAiCyL7OfH9/IHMdWRyU4Qwz79fzzDPMOWfOfOacat59l3MkIYQAERERkRVSyF0AERERkbkw6BAREZHVYtAhIiIiq8WgQ0RERFaLQYeIiIisFoMOERERWS0GHSIiIrJaDDpERERktRh0iIiIyGox6FCzt3LlSkiShEOHDsldSr3mz58PSZJqfXz22Wey1rZ48WKsXLmyxvLU1FRIklTruuYuOjoazs7ORsvqOg5NzRbOx7x58xAQEAA7Ozu4ubnddvs9e/ZgwoQJCAgIgEajgZOTE7p06YIXXngBp06dMto2Ojra6N8vJycnBAYGYsyYMVixYgXKysrM9K3IEtnJXQCRrdmyZQu0Wq3RsqCgIJmqqbJ48WJ4eXkhOjraaHnLli2xb98+tGvXTp7Cmlhdx8FS6rCW8/Hjjz/i3Xffxdy5czF8+HBoNJp6t583bx7effddREVFYd68ebjnnntQWVmJo0ePYtWqVViwYAEqKyuhVCoN73FwcMC2bdsAACUlJUhPT8fmzZvx1FNP4aOPPsKWLVvQunVrs35PsgwMOkRNLCIiAl5eXnKXYRKNRoPevXvLXUazJoRAaWkpHBwc7npf1nI+jh8/DgB47rnn4O3tXe+23377Ld59911Mnz4dixcvhiRJhnVDhw7FnDlzsHjx4hrvUygUNY7VpEmT8OSTT2LUqFF45JFHsH///kb4NmTp2HVFNmPPnj0YPHgwXFxc4OjoiD59+uDnn3822qa4uBgvvvgigoKCYG9vDw8PD/To0QPffvutYZsLFy7gscceg5+fHzQaDXx8fDB48GAcOXLkruqrr1tCkiTMnz/f8Lq6G+zEiROYMGECtFotfHx8MGXKFOTl5Rm9V6/X49NPP0V4eDgcHBzg5uaG3r17Y+PGjQCAwMBAnDhxAjt37jQ09QcGBtZbkynHsrpLcfv27ZgxYwa8vLzg6emJhx56CJcvX673WCxcuBCSJOHcuXM11r388stQq9XIzs4GACQlJWHUqFHw9vaGRqOBn58fRo4ciUuXLtX7Gbeq7zgAQH5+vuGfDbVajVatWmH27NkoKioy2o8kSXjmmWewdOlSdO7cGRqNBqtWrQIAvPnmm+jVqxc8PDzg6uqK7t2748svv8TN91a2hPOxbds2DBgwAJ6ennBwcEBAQAAefvhhFBcX13sM9Xo9PvjgA3Tq1AkajQbe3t6YNGmS0bkIDAzEvHnzAAA+Pj41/tm+1TvvvAMvLy98/PHHRiHn5uMdExNj1JpTn2HDhuGpp57CgQMHsGvXLpPeQ80bgw7ZhJ07d2LQoEHIy8vDl19+iW+//RYuLi4YPXo01q5da9huzpw5WLJkCZ577jls2bIFq1evxrhx45CTk2PYZsSIEUhMTMQHH3yA+Ph4LFmyBN26dcP169dNqkWn06GystLw0Ol0d/y9Hn74YXTo0AE//PADXnnlFXzzzTd4/vnnjbaJjo7GrFmzEBkZibVr12LNmjUYM2YMUlNTAQDr169H27Zt0a1bN+zbtw/79u3D+vXr6/xMU49ltWnTpkGlUuGbb77BBx98gB07duCJJ56o93s98cQTUKvVNX7QdTodvvrqK4wePRpeXl4oKirC0KFDkZmZibi4OMTHx2PhwoUICAhAQUGBaQfxhvqOQ3FxMfr3749Vq1bhueeew+bNm/Hyyy9j5cqVGDNmjFFQAYANGzZgyZIleP311/HLL7+gX79+AKqCyt/+9jd89913WLduHR566CE8++yzePvtt02qozaNfT5SU1MxcuRIqNVqLF++HFu2bMF7770HJycnlJeX13sMZ8yYgZdffhlDhw7Fxo0b8fbbb2PLli3o06ePIZiuX78eU6dOBVDVjbtv3z5Mmzat1v1dvnwZycnJGDp0KOzt7ev97IYYM2YMADDo2ApB1MytWLFCABAJCQl1btO7d2/h7e0tCgoKDMsqKytFSEiIaN26tdDr9UIIIUJCQsTYsWPr3E92drYAIBYuXNjgOt944w0BoMajVatWQgghUlJSBACxYsWKGu8FIN54440a+/rggw+Mtps5c6awt7c3fJ9du3YJAGLu3Ln11talSxfRv3//Gstrq8nUY1l9XmbOnGm0zw8++EAAEBkZGfXW9NBDD4nWrVsLnU5nWLZp0yYBQPz3v/8VQghx6NAhAUBs2LCh3n3VZvLkycLJycloWV3HITY2VigUihr/jP3nP/8RAMSmTZsMywAIrVYrrl27Vu/n63Q6UVFRId566y3h6elpOG711dEU56P6Ox05cqTe+m918uTJWvd/4MABAUC89tprhmXV//xevXq13n3u379fABCvvPJKjXWVlZWioqLC8Lj5+NV2bmurdcaMGaZ+PWrG2KJDVq+oqAgHDhzAI488YjTLRqlUYuLEibh06RJOnz4NAOjZsyc2b96MV155BTt27EBJSYnRvjw8PNCuXTv861//woIFC5CUlAS9Xt+gen799VckJCQYHps2bbrj71b9f6bVQkNDUVpaiqysLADA5s2bAQAxMTF3/Bk3a8ixrK9GALh48WK9n/Xkk0/i0qVL+PXXXw3LVqxYAV9fXwwfPhwA0L59e7i7u+Pll1/G0qVLkZycfFffry4//fQTQkJCEB4ebtQad//990OSJOzYscNo+0GDBsHd3b3GfrZt24YhQ4ZAq9VCqVRCpVLh9ddfR05OjuGcNYQ5zkd4eDjUajWefvpprFq1ChcuXDCplu3btwNAjQHUPXv2ROfOnfHbb7816LvdjqenJ1QqleHxww8/mPxecUsLHFk3Bh2yerm5uRBCoGXLljXW+fn5AYCha+qTTz7Byy+/jA0bNmDgwIHw8PDA2LFjcfbsWQBV4wF+++033H///fjggw/QvXt3tGjRAs8995zJXSVhYWHo0aOH4VH9Q3MnPD09jV5Xz16pDmhXr16FUqmEr6/vHX/GzRpyLE2tsS7Dhw9Hy5YtsWLFCsNnb9y4EZMmTTKMx9Bqtdi5cyfCw8Px2muvoUuXLvDz88Mbb7yBioqKO/uStcjMzMTRo0eNflhVKhVcXFwghDB0y1Sr7fgcPHgQw4YNAwB8/vnn+P3335GQkIC5c+cCuP3xqI05zke7du3w66+/wtvbGzExMWjXrh3atWuHRYsW1VtL9efUVcutdZjC398fQO2heMeOHUhISMDSpUsbvN/q/VUfI7JunHVFVs/d3R0KhQIZGRk11lUPwqyeBeXk5IQ333wTb775JjIzMw2tO6NHjzZcq6NNmzb48ssvAQBnzpzBd999h/nz56O8vPyO/qNbrXoMwq3X+LiTH4hqLVq0gE6nw5UrV2r9AWqohhzLu1XdKvHJJ5/g+vXr+Oabb1BWVoYnn3zSaLuuXbtizZo1EELg6NGjWLlyJd566y04ODjglVdeaZRavLy84ODggOXLl9e5/ma1DZpds2YNVCoVfvrpJ6PxJhs2bLjjusx1Pvr164d+/fpBp9Ph0KFD+PTTTzF79mz4+Pjgscceq/U91QEqIyOjxrTty5cv31Edfn5+6NKlC+Lj41FaWmp03MLDwwEAhYWFDd5v9UD8AQMGNPi91PywRYesnpOTE3r16oV169YZ/V+zXq/HV199hdatW6NDhw413ufj44Po6GhMmDABp0+frnXGSYcOHTBv3jx07doVhw8fvqs6fXx8YG9vj6NHjxot//HHH+94n9VdPEuWLKl3O41GY1KLwp0eyzv15JNPorS0FN9++y1WrlyJqKgodOrUqdZtJUlCWFgYPv74Y7i5ud3R+ajrOIwaNQrnz5+Hp6enUWtc9ePm2Vl1kSQJdnZ2RrODSkpKsHr1apPruJW5z4dSqUSvXr0QFxcHAPUe00GDBgEAvvrqK6PlCQkJOHnyJAYPHnxHNcydOxfZ2dmYM2dOo3Q5xcfH44svvkCfPn3Qt2/fu94fWT626JDV2LZtm2Em0c1GjBiB2NhYDB06FAMHDsSLL74ItVqNxYsX4/jx4/j2228N/wfeq1cvjBo1CqGhoXB3d8fJkyexevVqREVFwdHREUePHsUzzzyDcePG4Z577oFarca2bdtw9OjRu249kCQJTzzxBJYvX4527dohLCwMBw8exDfffHPH++zXrx8mTpyId955B5mZmRg1ahQ0Gg2SkpLg6OiIZ599FsD/WkXWrl2Ltm3bwt7eHl27dq11n6Yey8bQqVMnREVFITY2Funp6Vi2bJnR+p9++gmLFy/G2LFj0bZtWwghsG7dOly/fh1Dhw5t8OfVdRxmz56NH374Affddx+ef/55hIaGQq/XIy0tDVu3bsULL7yAXr161bvvkSNHYsGCBfjrX/+Kp59+Gjk5Ofjwww9rvVienOdj6dKl2LZtG0aOHImAgACUlpYaWrKGDBlS5/s6duyIp59+Gp9++ikUCgWGDx+O1NRU/OMf/4C/v3+N2YCmmjBhAk6cOIF3330Xf/zxB6Kjo3HPPfdAr9cjPT3dEBRdXFyM3qfX6w3XySkrK0NaWho2b96M7777Dp07d8Z33313R/VQMyTfOGiixlE9m6SuR0pKihBCiN27d4tBgwYJJycn4eDgIHr37m2YvVPtlVdeET169BDu7u5Co9GItm3biueff15kZ2cLIYTIzMwU0dHRolOnTsLJyUk4OzuL0NBQ8fHHH4vKysp66zRlpkleXp6YNm2a8PHxEU5OTmL06NEiNTW1zllXt+6r+lhUf2chqmb3fPzxxyIkJESo1Wqh1WpFVFSU0XdPTU0Vw4YNEy4uLgKAaNOmjRCi7plgphzLumbDbd++XQAQ27dvr/d4VVu2bJkAIBwcHEReXp7RulOnTokJEyaIdu3aCQcHB6HVakXPnj3FypUrb7vf2mbm1HUchBCisLBQzJs3T3Ts2NFwHLt27Sqef/55ceXKFcN2AERMTEytn7l8+XLRsWNHwz9bsbGx4ssvv6xxzuQ8H/v27RMPPvigaNOmjdBoNMLT01P0799fbNy48bbHVKfTiffff1906NBBqFQq4eXlJZ544gmRnp5utJ2ps65utmvXLjF+/HjRunVroVKphKOjowgODhYzZswQhw4dMtp28uTJRv8NcHBwEAEBAWL06NFi+fLloqyszOTPpeZPEoLDz4mIiMg6cYwOERERWS0GHSIiIrJaDDpERERktRh0iIiIyGox6BAREZHVYtAhIiIiq2XzFwzU6/W4fPkyXFxcGvVCZ0RERGQ+QggUFBTAz88PCkXd7TY2H3QuX75suHEcERERNS/p6ek17q92M5sPOtWXDU9PT4erq6vM1RAREZEp8vPz4e/vX+P2H7ey+aBT3V3l6urKoENERNTM3G7YCQcjExERkdVi0CEiIiKrxaBDREREVstmg05cXByCg4MRGRkpdylERERkJpIQQshdhJzy8/Oh1WqRl5fHwchERETNhKm/3zbbokNERETWj0GHiIiIrBaDDhEREVktBh0iIiKyWgw6REREZLUYdIiIiMhqMegQERGR1bL5m3qaS2Z+KSp0eng5a2CvUspdDhERkU1ii46ZTPh8P/q+vx1HL+XJXQoREZHNYtAhIiIiq8WgY2Y2focNIiIiWTHomIkkdwFERETEoENERETWi0HHzNhxRUREJB8GHTORJHZeERERyY1Bh4iIiKyWzQaduLg4BAcHIzIy0qyfw0lXRERE8rHZoBMTE4Pk5GQkJCSYZf/suCIiIpKfzQadpiI4HJmIiEg2DDpERERktRh0zISTroiIiOTHoGNu7LkiIiKSDYOOmUgcjkxERCQ7Bh0iIiKyWgw6ZsaeKyIiIvkw6JgJByMTERHJj0GHiIiIrBaDjpnxFhBERETyYdAhIiIiq8WgY2a8BQQREZF8GHSIiIjIajHomInEaVdERESyY9AxMw5GJiIikg+DDhEREVktBh0zYccVERGR/Bh0zIw9V0RERPJh0DETjkUmIiKSH4MOERERWS0GHTMTnHZFREQkGwYdM2HXFRERkfwYdMyM7TlERETysYqgY2dnh/DwcISHh2PatGlyl0NEREQWwk7uAhqDm5sbjhw5IncZRiReSYeIiEh2VtGiY9HYd0VERCQb2YPOrl27MHr0aPj5+UGSJGzYsKHGNosXL0ZQUBDs7e0RERGB3bt3G63Pz89HREQE+vbti507dzZR5URERGTpZA86RUVFCAsLw2effVbr+rVr12L27NmYO3cukpKS0K9fPwwfPhxpaWmGbVJTU5GYmIilS5di0qRJyM/Pb6ry68RZV0RERPKTPegMHz4c77zzDh566KFa1y9YsABTp07FtGnT0LlzZyxcuBD+/v5YsmSJYRs/Pz8AQEhICIKDg3HmzJk6P6+srAz5+flGD3MS7LsiIiKSjexBpz7l5eVITEzEsGHDjJYPGzYMe/fuBQDk5uairKwMAHDp0iUkJyejbdu2de4zNjYWWq3W8PD39zffFyAiIiJZWXTQyc7Ohk6ng4+Pj9FyHx8fXLlyBQBw8uRJ9OjRA2FhYRg1ahQWLVoEDw+POvf56quvIi8vz/BIT083S+3suSIiIpJfs5heLt0y4EUIYVjWp08fHDt2zOR9aTQaaDSaRq2vPrwDBBERkXwsukXHy8sLSqXS0HpTLSsrq0Yrj8XhaGQiIiLZWXTQUavViIiIQHx8vNHy+Ph49OnT5672HRcXh+DgYERGRt7Vfm6HLTpERETykb3rqrCwEOfOnTO8TklJwZEjR+Dh4YGAgADMmTMHEydORI8ePRAVFYVly5YhLS0N06dPv6vPjYmJQUxMDPLz86HVau/2axAREZEFkj3oHDp0CAMHDjS8njNnDgBg8uTJWLlyJcaPH4+cnBy89dZbyMjIQEhICDZt2oQ2bdrIVbJJ2HFFREQkP9mDzoABAyBu078zc+ZMzJw5s4kqalzsuSIiIpKPRY/RISIiIrobDDpmwklXRERE8rPZoNN0s67YeUVERCQXmw06MTExSE5ORkJCgtylEBERkZnYbNAxN/ZcERERyY9Bx8zYcUVERCQfBh0iIiKyWgw6ZlJ901GORSYiIpKPzQadppp1RURERPKx2aBj7llXHIxMREQkP5sNOk2HfVdERERyYdAhIiIiq8WgYya8BQQREZH8GHTMjLOuiIiI5GOzQYezroiIiKyfzQYd88+6Yt8VERGR3Gw26DQV9lwRERHJh0GHiIiIrBaDjrnc6LniYGQiIiL5MOgQERGR1WLQISIiIqvFoGMm1XOuBIcjExERycZmgw6vo0NERGT9bDbomP06OryMDhERkexsNug0Fc66IiIikg+DDhEREVktBh0z4S0giIiI5MegY2bsuSIiIpIPgw4RERFZLQYdM5EMt4Bgmw4REZFcGHSIiIjIajHoEBERkdWy2aBj7isj84KBRERE8rPZoGPuKyMTERGR/Gw26BAREZH1Y9Axk+oLBnLSFRERkXwYdIiIiMhqMeiYCQcjExERyY9Bx8wEbwJBREQkGwYdIiIisloMOmbGwchERETyYdAhIiIiq8WgQ0RERFaLQcdMJInX0SEiIpIbgw4RERFZLZsNOua+qScRERHJz2aDjrlv6ll9vUD2XBEREcnHZoMOERERWT8GHTMTHI1MREQkGwYdM+G9roiIiOTHoENERERWi0HHTDgYmYiISH4MOkRERGS1GHSIiIjIajHomEn1LSDYd0VERCQfBh0iIiKyWgw6REREZLUYdMzkf7Ou2HdFREQkFwYdIiIisloMOmbGO0AQERHJh0HHTHgLCCIiIvkx6BAREZHVYtAxM/ZcERERycdmg05cXByCg4MRGRlppk9g3xUREZHcbDboxMTEIDk5GQkJCXKXQkRERGZis0HH3Ax3gGDfFRERkWwYdIiIiMhqMegQERGR1WLQMRPeAoKIiEh+DDpERERktRh0zIyDkYmIiOTDoGMmvAUEERGR/Bh0iIiIyGox6JgZe66IiIjkw6BjJhJvAUFERCQ7Bh0iIiKyWgw65sZpV0RERLJh0DETzroiIiKSH4MOERERWa1GCzpCCGRlZTXW7po9w93L5S2DiIjIppkcdBwdHXH16lXD6wceeAAZGRmG11lZWWjZsmXjVkdERER0F0wOOqWlpRA3Daz9/fffUVJSYrSN4MDbGnhIiIiI5NOoY3QkjsA14HV0iIiI5MfByERERGS1TA46kiQZtdjc+ppqx+48IiIi+diZuqEQAh06dDCEm8LCQnTr1g0KhcKwnm7CDEhERCQ7k4POihUrzFkHERERUaMzOehMnjzZnHVYLbZzERERyeeuBiOXlpZi1apVWLx4Mc6ePdtYNd2R4uJitGnTBi+++KKsdVRjzxUREZH8TG7R+fvf/47y8nIsWrQIAFBeXo6oqCicOHECjo6OeOmllxAfH4+oqCizFVufd999F7169ZLls4mIiMgymdyis3nzZgwePNjw+uuvv8bFixdx9uxZ5ObmYty4cXjnnXfMUuTtnD17FqdOncKIESNk+fz6cIw2ERGRfEwOOmlpaQgODja83rp1Kx555BG0adMGkiRh1qxZSEpKanABu3btwujRo+Hn5wdJkrBhw4Ya2yxevBhBQUGwt7dHREQEdu/ebbT+xRdfRGxsbIM/25w49Z6IiEh+JgcdhUJhNIV8//796N27t+G1m5sbcnNzG1xAUVERwsLC8Nlnn9W6fu3atZg9ezbmzp2LpKQk9OvXD8OHD0daWhoA4Mcff0SHDh3QoUOHBn92U2CDDhERkXxMHqPTqVMn/Pe//8WcOXNw4sQJpKWlYeDAgYb1Fy9ehI+PT4MLGD58OIYPH17n+gULFmDq1KmYNm0aAGDhwoX45ZdfsGTJEsTGxmL//v1Ys2YNvv/+exQWFqKiogKurq54/fXXa91fWVkZysrKDK/z8/MbXLMp2J5DREQkP5NbdP7+97/jlVdeweDBgzF48GCMGDECQUFBhvWbNm1Cz549G7W48vJyJCYmYtiwYUbLhw0bhr179wIAYmNjkZ6ejtTUVHz44Yd46qmn6gw51dtrtVrDw9/fv1FrJiIiIsthctB5+OGHsWnTJoSGhuL555/H2rVrjdY7Ojpi5syZjVpcdnY2dDpdjZYiHx8fXLly5Y72+eqrryIvL8/wSE9Pb4xSa6geosMrRhMREcnH5K4rABgyZAiGDBlS67o33nijUQqqza0De4UQtQ72jY6Ovu2+NBoNNBpNY5VWJ1d7FQAgr6TC7J9FREREtTM56FQP/r2dgICAOy7mVl5eXlAqlTVab7Kysu5oPFBT8nRWAwCyC8tusyURERGZi8lB5+bxONXdMTe3qlS3suh0ukYrTq1WIyIiAvHx8XjwwQcNy+Pj4/GXv/zlrvYdFxeHuLi4Rq33Zu1aOAMA9p7PqbMFioiIiMzL5KAjSRJat26N6OhojB49GnZ2Der1qlNhYSHOnTtneJ2SkoIjR47Aw8MDAQEBmDNnDiZOnIgePXogKioKy5YtQ1paGqZPn35XnxsTE4OYmBjk5+dDq9Xe7deoYXBnbziqlbiYU4yk9OvoHuDe6J9BRERE9TM5rVy6dAmrVq3CypUrsXTpUjzxxBOYOnUqOnfufFcFHDp0yGia+pw5cwBU3UR05cqVGD9+PHJycvDWW28hIyMDISEh2LRpE9q0aXNXn2tujmo7PNDFF+uS/sSGpD8ZdIiIiGQgiTuYFrRnzx6sWLEC33//PYKDgzF16lRMnToVCsVd3SNUFtUtOnl5eXB1dW3Ufe86cxWTlh+Eu6MKB+cOgUrZ/I4PERGRJTL19/uOfnn79u2LL7/8EmfPnoWjoyOmT5+O69ev32mtVqtPO0+0cNEgt7gCu85clbscIiIim3NHQWfv3r2YNm0aOnTogMLCQsTFxcHNza2RS2v+7JQKjAnzAwCsT/pT5mqIiIhsj8lBJyMjA++//z46deqEBx98EK6urti7dy8OHjyI6dOnN7tuq7i4OAQHByMyMtKsn/Ngt1YAgK3Jmcgr5jV1iIiImpLJY3TUajX8/PwwefJkjBkzBiqVqtbtQkNDG7VAczPnGB2gatr98EW7cepKAeaPDkb0vUG3fxMRERHVy9Tfb5ODzs0tNtXXhLn1rY19HZ2mYO6gAwCr9qbijY0n0MnXBZtn9eM1dYiIiO6Sqb/fJk8vT0lJaZTCbNHY8Fb456aTOHWlAH9cykO4v5vcJREREdkEk4OOpV+3xpJpHVUY0bUl1if9ibUJaQw6RERETaR5jSBuxh6L9AcAbDxyGUVllTJXQ0REZBtsNug01ayraj2DPNDWywlF5Tr894/LTfKZREREts5mg05MTAySk5ORkJDQJJ8nSRIe61nVqvPvfRdrDOQmIiKixmezQUcOj/bwh71KgeSMfBxMuSZ3OURERFaPQacJuTmq8WC31gCAlXtT5S2GiIjIBpg866pat27dar0OjCRJsLe3R/v27REdHW10R3L6n+g+gfj2YBp+OXEFf14vQSs3B7lLIiIisloNbtF54IEHcOHCBTg5OWHgwIEYMGAAnJ2dcf78eURGRiIjIwNDhgzBjz/+aI56m72Ovi7o084TegGs3ndR7nKIiIisWoODTnZ2Nl544QXs3r0bH330ERYsWIBdu3bhxRdfRFFREbZu3Yp58+bh7bffNke9jaapZ13dLLpPIABgTUIaSsqb15WkiYiImhOTbwFRTavVIjExEe3btzdafu7cOURERCAvLw+nTp1CZGQkCgoKGrVYc2iKW0DcSqcXGPDhdqRfK8HbY0MwsTcvxkhERNQQpv5+N7hFx97eHnv37q2xfO/evbC3twcA6PV6aDSahu7aZigVEqb1bQsAWLbrPCp1epkrIiIisk4NHoz87LPPYvr06UhMTERkZCQkScLBgwfxxRdf4LXXXgMA/PLLL+jWrVujF2tNHu3hj0W/nUX6tRL8fCwDfwlvJXdJREREVqfBXVcA8PXXX+Ozzz7D6dOnAQAdO3bEs88+i7/+9a8AgJKSEsMsLEsnR9dVtU9/O4uP4s/wruZEREQNZOrv9x0FHWsiZ9DJK65An/d+Q1G5DiuiIzGwk3eTfj4REVFzZbYxOtXKy8tx6dIlpKWlGT3IdFpHFR6/MRB5yY7zMldDRERkfRocdM6ePYt+/frBwcEBbdq0QVBQEIKCghAYGIigoCBz1GjVpvYNglqpwMHUazhwIUfucoiIiKxKgwcjR0dHw87ODj/99BNatmzZbMeVxMXFIS4uDjqdvNex8XG1x7gerfH1gTR8FH8Ga5/u3WyPKRERkaVp8BgdJycnJCYmolOnTuaqqUnJOUanWkZeCfr/awfKK/X4elov3NveS5Y6iIiImguzjdEJDg5Gdnb2XRVHxlpqHfDXngEAgI+2noaNjw8nIiJqNA0OOu+//z5eeukl7NixAzk5OcjPzzd60J2ZOaAdNHYKHE67jp1nrspdDhERkVVocNeVQlGVjW4dRyKEgCRJso95aShL6Lqq9u7Pyfh8dwpCW2vxY8y9HKtDRERUB1N/vxs8GHn79u13VRjV7W/92+HrA2k4eikPv5zIxAMhvnKXRERE1Kw1OOj079/fHHUQAC9nDab2DcKn287h/S2nMLizN1TKO77UERERkc0zKegcPXoUISEhUCgUOHr0aL3bhoaGNkphtupv/dvh24NpSMkuwjcH0jC5T6DcJRERETVbJo3RUSgUuHLlCry9vaFQKCBJUq0zgzhGp3F8tf8i5m04DndHFXa+NBCu9iq5SyIiIrIojTpGJyUlBS1atDD8Teb1WKQ/VvyegvNXi7Bkx3m8/IB1XLOIiIioqdnsTT1vvjLymTNnLKpFBwB+Tc7EtH8fgtpOge0vDkArNwe5SyIiIrIYZr17+ZkzZ7Bjxw5kZWVBr9cbrXv99dcbXq2MLLHrCqiarj/h8/3Yf+EaRof54dMJ3eQuiYiIyGKYLeh8/vnnmDFjBry8vODr62t0rRdJknD48OE7r1oGlhp0AOD4n3kY/dkeCAF8+1RvRLXzlLskIiIii2C2oNOmTRvMnDkTL7/88l0XaQksOegAwLwNx/DV/jR09HHBT8/15XRzIiIimPFeV7m5uRg3btxdFUeme3FYR7g7qnA6swCr912UuxwiIqJmpcFBZ9y4cdi6das5aqFauDmq8dKNWVcfx59BVkGpzBURERE1Hw2+MnL79u3xj3/8A/v370fXrl2hUhlf4+W5555rtOKoyqM9/PHtwapbQ7y/+TQ+ejRM7pKIiIiahQaP0QkKCqp7Z5KECxcu3HVRTcnSx+hUS0rLxYOL9wIA1j7dG73acmAyERHZLrPd1JMXDJRHtwB3TOgZgG8PpuHV9cew6bl+sFcp5S6LiIjIonEKTzPyyvBOaOGiwYWrRYjbfk7ucoiIiCyeSS06c+bMwdtvvw0nJyfMmTOn3m0XLFjQKIVRTVoHFd4a0wUzvj6MJTvOY1SoHzr6ushdFhERkcUyKegkJSWhoqLC8Hddbr54IJnHAyG+GBrsg/jkTLz8w1H8MKMPlAoedyIiotrY7L2uqjWXwcg3u5JXiiELdqKwrBLzRwcj+t66B4gTERFZI7NdMNBaxMXFITg4GJGRkXKX0mC+Wnu8/EBHAMC/fjmN9GvFMldERERkme6oRSchIQHff/890tLSUF5ebrRu3bp1jVZcU2iOLToAoNcLjF+2Dwmpuejd1gPfTOsNBbuwiIjIRpitRWfNmjW49957kZycjPXr16OiogLJycnYtm0btFrtXRVNplMoJHw4LgwOKiX2X7iGf+9LlbskIiIii9PgoPPPf/4TH3/8MX766Seo1WosWrQIJ0+exKOPPoqAgABz1Eh1aOPphNdGVN0e4r0tp3DhaqHMFREREVmWBged8+fPY+TIkQAAjUaDoqIiSJKE559/HsuWLWv0Aql+j/dqg77tvVBaoccL3/8Bnd6mx5YTEREZaXDQ8fDwQEFBAQCgVatWOH78OADg+vXrKC7moNimplBIeP+RULho7JCUdh3LdjWvW3AQERGZU4ODTr9+/RAfHw8AePTRRzFr1iw89dRTmDBhAgYPHtzoBdLttXJzwOujgwFU3eH81JV8mSsiIiKyDA2edXXt2jWUlpbCz88Per0eH374Ifbs2WO4q7m7u7u5ajWL5jrr6lZCCDz170P49WQWOvq44Mdn7uW9sIiIyGqZ+vvdoKBTWVmJr7/+Gvfffz98fX0bpVC5WUvQAYDswjI8sHA3sgvLEN0nEPPHdJG7JCIiIrMwy/RyOzs7zJgxA2VlZXddIDU+L2cNPhwXCgBYuTcVv53MlLkiIiIieTV4jE6vXr3qvd8VyWtAR29M7Vt1S4i//+cosvJLZa6IiIhIPibd1PNmM2fOxAsvvIBLly4hIiICTk5ORutDQ0MbrTi6My890BF7z+fgZEY+Xvj+D6x6sievmkxERDbJ5DE6U6ZMwcKFC+Hm5lZzJ5IEIQQkSYJOp2vsGs3Kmsbo3OxcVgFGfboHpRV6zB3RGU/d11bukoiIiBpNow9GViqVyMjIQElJSb3btWnTpmGVysxagw4AfHMgDa+tPwaVUsJ/pvdBmL+b3CURERE1ClN/v03uuqrOQ80tyNiyCT39sevMVWw5cQUx3xzGz8/2g9ZRJXdZRERETaZBg5ElieM8mhNJqrpqcoCHIy7lluCF7//AHdysnoiIqNlqUNDp0KEDPDw86n2QZdE6qLD48e5QKxX49WQmvtidIndJRERETaZBs67efPNNaLVac9VCZhLSSovXRwdj3objeG/LKXQLcEOPQIZSIiKyfiYPRlYoFLhy5Qq8vb3NXVOTiIuLQ1xcHHQ6Hc6cOWOVg5FvJoTArDVHsPGPy/B1tcfPz/WFp7NG7rKIiIjuiNlmXVlL0KlmzbOublVUVokxn+3B+atF6HePF6+vQ0REzVaj3wKCg1ibPyeNHRY/HgF7lQK7z2bjk21n5S6JiIjIrEwOOnq93upac2xRR18XvDu2KwBg0W9neT8sIiKyag2+1xU1fw9HtMakqDYQApi95gguXC2UuyQiIiKzYNCxUfNGBiMy0B0FZZV4enUiCssq5S6JiIio0THo2Ci1nQJxj3eHj6sG57IK8cJ3R6DXcxwWERFZFwYdG+btYo8lT0RArVTglxOZWLLzvNwlERERNSoGHRvXPcAdb/2lCwDgw62nsf10lswVERERNR4GHcJjPQPw114BEAKY9W0SUrKL5C6JiIioUTDoEADgjdHB6B7ghvzSSkxdlYC8kgq5SyIiIrprDDoEANDYKbH0iQi01NrjwtUiPPPNYVTq9HKXRUREdFcYdMjA29Uen0/qAQeVErvPZuOdn0/KXRIREdFdYdAhIyGttPh4fDgAYOXeVKzef1HegoiIiO4Cgw7V8ECIL/5+f0cAwPyNJ7DnbLbMFREREd0ZBh2q1cwB7fBgt1bQ6QVmfp3I20QQEVGzxKBDtZIkCbEPdb1pJtYhXC8ul7ssIiKiBmHQoTrZq5T4v4k90MrNASnZRZj+VSLKKnVyl0VERGQyBh2qVwsXDb6Y3APOGjvsv3ANL/3nKO+JRUREzQaDDt1W55auWPJEd9gpJPx45DI+3Hpa7pKIiIhMwqBDJul3TwvEPtQVALB4x3l8fYDTzomIyPIx6JDJxvXwx+wh9wAA/rHhOLaf4g1AiYjIsjHoUIPMGnwPHoloDb0AYr45jGOX8uQuiYiIqE4MOtQg1dPO+93jheJyHaasSkD6tWK5yyIiIqoVgw41mEqpwOLHu6OTrwuuFpQhesVB5BbxGjtERGR5mn3QKSgoQGRkJMLDw9G1a1d8/vnncpdkE1zsVVj5ZE+01Nrj/NUiRK9MQFFZpdxlERERGZGEEM36oig6nQ5lZWVwdHREcXExQkJCkJCQAE9PT5Pen5+fD61Wi7y8PLi6upq5WutzLqsAjyzdh+vFFeh3jxe+nBwJtV2zz89ERGThTP39bva/SEqlEo6OjgCA0tJS6HQ6NPPs1qy093bBiuhIOKqV2H02G3O+OwIdLyhIREQWQvags2vXLowePRp+fn6QJAkbNmyosc3ixYsRFBQEe3t7REREYPfu3Ubrr1+/jrCwMLRu3RovvfQSvLy8mqh6AoBuAe5Y+kQEVEoJPx3NwPyNJxg2iYjIIsgedIqKihAWFobPPvus1vVr167F7NmzMXfuXCQlJaFfv34YPnw40tLSDNu4ubnhjz/+QEpKCr755htkZmY2Vfl0w30dWmDBo+GQJGD1/otY+OtZuUsiIiKyrDE6kiRh/fr1GDt2rGFZr1690L17dyxZssSwrHPnzhg7dixiY2Nr7GPGjBkYNGgQxo0bV+tnlJWVoayszPA6Pz8f/v7+HKPTSP69LxWv/3gCAPDmmC6Y3CdQ3oKIiMgqWcUYnfLyciQmJmLYsGFGy4cNG4a9e/cCADIzM5Gfnw+g6kvv2rULHTt2rHOfsbGx0Gq1hoe/v7/5voANmhQViFmDq66ePP+/J7Ah6U+ZKyIiIltm0UEnOzsbOp0OPj4+Rst9fHxw5coVAMClS5dw3333ISwsDH379sUzzzyD0NDQOvf56quvIi8vz/BIT08363ewRbOH3INJUW0gBPDC939g87EMuUsiIiIbZSd3AaaQJMnotRDCsCwiIgJHjhwxeV8ajQYajaYxy6NbSJKE+aO7oLhch/8kXsJza5LwfyoFBnXyuf2biYiIGpFFt+h4eXlBqVQaWm+qZWVl1WjlIcuiUEh4/+FQjAptiQqdwPSvDmPP2Wy5yyIiIhtj0UFHrVYjIiIC8fHxRsvj4+PRp0+fu9p3XFwcgoODERkZeVf7obopFRI+Hh+OYcE+KK/U46l/H8LBlGtyl0VERDZE9qBTWFiII0eOGLqfUlJScOTIEcP08Tlz5uCLL77A8uXLcfLkSTz//PNIS0vD9OnT7+pzY2JikJycjISEhLv9ClQPlVKBT//aDf07tEBJhQ5TVibgSPp1ucsiIiIbIfv08h07dmDgwIE1lk+ePBkrV64EUHXBwA8++AAZGRkICQnBxx9/jPvuu69RPp+3gGgapRU6PLkiAfsu5MDV3g7fPt0bXfy0cpdFRETNlKm/37IHHbkx6DSdorJKTFp+EIkXc+HuqMJX03ox7BAR0R2xiuvokHVx0thhxZORCPN3Q25xBR7/4gCO/5knd1lERGTFGHSoSbnaq7B6ak90C3DD9Rth59glhh0iIjIPmw06nHUlH1d7Ff49pSe6B7ghr6QCj3+xH39wgDIREZkBx+hwjI5sCssqEb38IA5dzIWLvR3+PaUnugW4y10WERE1AxyjQxbPWWOHlVN6omegBwpKKzHpy4M4nJYrd1lERGRFGHRIVs43Bij3DPJAQVlV2Nl/IUfusoiIyEow6JDsnDR2WPlkJPq080RhWSUmLz+I7aey5C6LiIisAIMOWQRHtR2WR0diSGdvlN24XcR//7gsd1lERNTM2WzQ4awry2OvUmLJExEYE+aHSr3Ac2uSsOZgmtxlERFRM8ZZV5x1ZXF0eoF//Hgc3xyoCjlzR3TGU/e1lbkqIiKyJJx1Rc2WUiHh3bEh+Fv/qnDz7qaTWLD1NGw8kxMR0R1g0CGLJEkSXnmgE/5+f0cAwCfbzmHehuPQ6Rl2iIjIdAw6ZLEkSULMwPZ4+y9dIEnA1wfSMP2rRJSU6+QujYiImgkGHbJ4E6MCsfiv3aG2UyA+OROPf7EfuUXlcpdFRETNgM0GHc66al6Gd22Jr6b2gqu9HQ6nXcfDS/ci/Vqx3GUREZGF46wrzrpqVs5kFiB6+UFczitFCxcNVj4ZiS5+WrnLIiKiJsZZV2SVOvi4YN3Me9HJ1wVXC8ow/v/2Y+eZq3KXRUREFopBh5odX609vpsehd5tPVBYVoknVxzEv/elyl0WERFZIAYdapZc7VVYNaUnHu7eGnoBvP7jCbzx43FU6vRyl0ZERBaEQYeaLY2dEh+OC8VLD1Rda2fVvouYsuoQ8ksrZK6MiIgsBYMONWuSJGHmgPZY+kR32KsU2HXmKh5ezBlZRERUhUGHrMIDIS3x/d/6wMdVg7NZhfhL3O84cCFH7rKIiEhmNht0eB0d69O1tRY/xvRFSCtXXCsqx+NfHMCK31N4jywiIhvG6+jwOjpWp6Rch5d/OIqNf1wGADzUrRXefbArHNRKmSsjIqLGwuvokM1yUCux6LFwzBvZGUqFhHVJf+IRXkmZiMgmMeiQVZIkCdP6tcXqqT3h4aTGicv5GPPZHuw5my13aURE1IQYdMiq9Wnnhf8+2xddW2mRW1yBScsP4LNtZ6HX23SPLRGRzWDQIavXys0B30+PwiMRVRcX/HDrGUxecRDZhWVyl0ZERGbGoEM2wV6lxL8eCcUHj4TCXqXA7rPZGLFoN/ad5xR0IiJrxqBDNkOSJDzawx8bn+mL9t7OyCoow+Nf7MeiX89Cx64sIiKrxKBDNqeDjws2PnOvoSvr41/PYNLyA8jML5W7NCIiamQMOmSTHNV2+HBcGD4aFwYHlRK/n8vB/Qt3YfOxDLlLIyKiRmSzQYdXRiYAeDiiNf77bNXVlK8XV2DG14fxwnd/oIA3BiUisgq8MjKvjEwAyiv1WPjrGSzZeR5CAK3dHbDg0XD0DPKQuzQiIqoFr4xM1ABqOwVeeqATvvtbFFq7O+BSbgnGL9uH97ecQlmlTu7yiIjoDjHoEN0kMtADm2f1wyMRrSEEsGTHeYz6ZA+S0nLlLo2IiO4Agw7RLVzsVfhwXBiWPtEdXs5qnM0qxMNL9uKdn5JRUs7WHSKi5oRBh6gOD4S0RPzz/fFgt1bQC+CLPSl4YNEu7L/AiwwSETUXDDpE9XB3UuPj8eFYER2Jllp7XMwpxmPL9uO19ceQV8yZWURElo5Bh8gEAzt545fn78OEngEAgG8OpGHwgh34IfESbHziIhGRRWPQITKRq70KsQ91xbdP9UZ7b2dkF5bjhe//wPhl+3H6SoHc5RERUS0YdIgaKKqdJzY91w8vP9AJDiolDqZcw8hPduOfm06iqKxS7vKIiOgmDDpEd0Btp8CMAe3w6wv9cX8XH1TqBZbtuoBBH+3AfxIvQc+bhBIRWQReGZlXRqZGsO1UJt7YeALp10oAACGtXPGPkcHo1dZT5sqIiKyTqb/fDDoMOtRISit0WLk3FZ9tO4fCG11YD3TxxasjOqGNp5PM1RERWRcGnduIi4tDXFwcdDodzpw5w6BDjSa7sAwfx5/BtwfToBeASilhUlQgZg5oB09njdzlERFZBQYdE7FFh8zlTGYB3vn5JHaduQoAcFIrMaVvEKb1awutg0rm6oiImjcGHRMx6JC57TpzFf/65TSO/ZkHANA6qPC3/m0R3ScQjmo7masjImqeGHRMxKBDTUEIgV9OXMFHW8/gbFYhAMDLWYMZA9rhrz0D4KBWylwhEVHzwqBjIgYdako6vcCPR/7Ewl/PIu1aMQDA00mNKX2DMDGqDVzt2aVFRGQKBh0TMeiQHCp0enx/6BKW7DxnmJLuYm+HyVGBmNI3CB5OapkrJCKybAw6JmLQITlV6vTY+MdlLN5xHududGk5qJR4rKc/ptwbBH8PR5krJCKyTAw6JmLQIUug1wtsTb6Cz7afw/E/8wEACgkYFuyLKX2DEBnoDkmSZK6SiMhyMOiYiEGHLIkQArvOZuOL3Rew+2y2YXnXVlpM7RuEEV1bQm3HO7cQETHomIhBhyzVmcwCrPg9BesO/4mySj0AwNtFg/GR/hgf6Y/W7uzWIiLbxaBjIgYdsnQ5hWX49mAa/r3vIrIKygAAkgQM6NACE3oGYFAnb9gp2cpDRLaFQcdEDDrUXJRX6rE1+Qq+OZCGvedzDMt9Xe3xaKQ/xkW05uBlIrIZDDomYtCh5igluwhrDqbh+8RLuFZUbljeM8gDD3VrhRGhLXlNHiKyagw6JmLQoeasrFKHrScysSahqpWn+t9mjZ0CQ4J98FC3VrivQwuo2LVFRFaGQcdEDDpkLTLySrAh6TLWHb5kuM0EAHg4qXF/F1+M7NoSvdt6cDwPEVkFBh0TMeiQtRFC4MTlfKw7/Cc2/vEnsgv/17Xl7qjC/V18MaJrS0S182RLDxE1Www6JmLQIWtWodNj/4UcbDqWgV9OZBqN53FzVGFIZx8M6eyNvve0gLOGd1InouaDQcdEDDpkKyp1ehxIuXYj9FwxaulRKSX0buuJQZ28MbiTDwI8OXuLiCwbg85txMXFIS4uDjqdDmfOnGHQIZui0wscSMnBbyez8NvJTKTmFButb+/tjEGdvNG3vRd6BnnAXqWUqVIiotox6JiILTpEwIWrhdh2Kgu/nczCwdRr0On/958FtZ0CPdq44972Xujb3gshrbRQKnjfLSKSF4OOiRh0iIzllVRg15mr2HXmKvacy0ZGXqnReq2DCn3aeaJ3W0/0CHRHJ19XBh8ianIMOiZi0CGqmxACF7KL8Pu5bOw5m41953NQUFZptI2LvR0i2rgjMtADkYEeCG2tZVcXEZkdg46JGHSITFep0+Pon3nYey4bB1NzcfhiLgpvCT5qpQJh/lp0D3BHaGs3hPlr0crNAZLEVh8iajwMOiZi0CG6c5U6PU5dKUBC6jUkpF7DwZRcZBeW1djOy1ldFXpuBJ+w1m5wd1LLUDERWQsGHRMx6BA1HiEELuYUIyH1Gv64dB1/pOfhZEY+KvU1/zPj7+GA4Jau6HzjEdzSFa3d2fJDRKZh0DERgw6ReZVW6JCckY8/0q9XPS7lISW7qNZtXezt0NnXFcF+rujc0gWdW7qiXQtnOPFihkR0CwYdEzHoEDW9vOIKHL9c1dqTnJGPkxkFOJdVgApd7f85auXmgPbezmjv7Yx7bjy393aGmyO7v4hsFYOOiRh0iCxDeaUe57IKcTIjv+pxJR+nMgqQc9NtK27l5axBe28ntPd2RqCnE9p4OiHQ0xH+Ho6c+UVk5Rh0TMSgQ2TZrhWV41xW4f8eVwtxLrMAl2+5vs/NJAlo6WqPAE9HowDUxtMJAZ6OvK8XkRVg0DERgw5R81RUVonzVwtxNrMQ568W4uK1YlzMKcLF7OIa1/q5ldZBhVZuDmjl7oBWbg5ofeO5+rWHk5qDooksnKm/3/zfGiJqlpw0dght7YbQ1m5Gy4UQyC2uQGpOES7mFCE1uyoApeZUPecWVyCvpOqRnJFf674dVEr4udmjlbsj/LT28HGtfmgMf3s6qaHgFaGJLB6DDhFZFUmS4OGkhoeTGt0D3GusLyitwOXrpfjzejH+zC3Bpesl+DO3BH/eeM4qKENJhQ7nrxbh/NXaZ4cBgJ1CgreLBj5ae/i42MNXaw9vVw18Xe3h7WIPT2c1vJw18HBS8xYZRDJi0CEim+Jir0JHXxU6+rrUur6sUoeM66WG4JORV4rMglJk3ni+kleGnKIyVOoFLueV1jtWCKgaL+ThqDYEH09nDbxu/O3lrIankwZeLhp4OqnRwkXDQdREjYxBh4joJho7JQK9nBDo5VTnNhU6PbILy3AlrxSZ+WXIzC9FZn4prtx4vlpQhpzCclwrLocQQE5ROXKKynEms/C2n++gUsLdUQWtoxrujiq4O6rhdsuzu5MKbo7qqr8dVXC1V7EbjagODDpERA2kUirQUuuAllqHerer1OmRW1yB7MKq4JNTVFYVgorKkV39fGPd1cIylFfqUVKhQ0me7rYtRTdTSFUDrN0d1dDeCD6uDiq42Nvd+Lvq2cXeDq4OVeu1DnZwsa/6216l4OBrsloMOkREZmKnVKCFiwYtXDS33VYIgYKySlwvqkBucTlyi8txvbj67wpcv+m5evn14goUllVCL4Dc4grkFlfcUZ0qpVQjCLnY28FZYwcnzc3PSjjVs8xJbcfxSGRxGHSIiCyAJFWFDVd7FQI8HU1+X3mlHtdLboSfonJcL6lAQWkl8ksqkF9agfySShSU/u/v/Bt/V2+jF0CFThi61+6Wg0pZbyhyUNnBUa2Eg1oJB1XVs6NaCXtV1fP//rYzrHdQKaG2U9x1bWSbGHSIiJoxtZ0C3i5VM70aSgiBonKdIRTdGpAKyypRdONRWKar+rv85uU6w9/VN24tqdChpEKH7NsPR2oQO4VkCD03B6OqZTfCk0oJjUoBe5USGrv/PWvsFNDUWKaEvarqubb32CkkdudZCQYdIiIbJUkSnG+0uPih/vFG9RFCoKxSbxx+ymsPSsXlOpRW6FBcXomSCj1KyitRUqFDcbkOJeU6w9+l5ToUV+iguxGgKvUCBaWVKCit/2KQjUUhoZbAZByO1EoFVEoF1HZVD5WyKlSplJLhtdquajvDa6UCqhvLNDdtU/0eo22N9qlgt+AdYtAhIqK7IkkS7FVVrSyezo23XyEEKnTipgB0c1CqWmYUjip0KKvQobRSX/VcoUdZpQ5llfqqdTc9G/194z3llXrDZ+sFUFxetV/gzsY+NTalQqoKRDcClp1Sgp2iKiRVrTNeZqeoeq1SVrVQVYclO6UE1S3r7JS3vkeCssZ+6t633Y3t7RRVtVTvs/p1CxcNVEp5uh8ZdIiIyCJJkgS1XVVLhxYqs3+eXi9QrtOj7EZAKr3l+dbAVKHTo1wnUF6pr/r7Rliq0FUFqeplVdvpUV5Ztf+KyqrXhvfc8t6q56ptb6bTC+j0AqUV+jq+geX67YX+aNeiEVNwAzDoEBERAVAoJNgrlDcu2mj+YHU71S1aN4ej6mBUqROo0OlRqReo1FUFI51eoEJfta5Sp0fFjXWVuv8tr9DpodMLVOpvvF9X33sEdPqqfVfe+KyKm9bdvG/9jX1W6qqedXr9jeeq13Yydrsx6BAREVmgm1u0cPsrFFAdmv18vfT0dAwYMADBwcEIDQ3F999/L3dJREREZCGafYuOnZ0dFi5ciPDwcGRlZaF79+4YMWIEnJzqvnw7ERER2YZmH3RatmyJli1bAgC8vb3h4eGBa9euMegQERGR/F1Xu3btwujRo+Hn5wdJkrBhw4Ya2yxevBhBQUGwt7dHREQEdu/eXeu+Dh06BL1eD39/fzNXTURERM2B7EGnqKgIYWFh+Oyzz2pdv3btWsyePRtz585FUlIS+vXrh+HDhyMtLc1ou5ycHEyaNAnLli1rirKJiIioGZCEEELuIqpJkoT169dj7NixhmW9evVC9+7dsWTJEsOyzp07Y+zYsYiNjQUAlJWVYejQoXjqqacwceLEej+jrKwMZWVlhtf5+fnw9/dHXl4eXF1dG/cLERERkVnk5+dDq9Xe9vdb9had+pSXlyMxMRHDhg0zWj5s2DDs3bsXQNV1BqKjozFo0KDbhhwAiI2NhVarNTzYzUVERGS9LDroZGdnQ6fTwcfHx2i5j48Prly5AgD4/fffsXbtWmzYsAHh4eEIDw/HsWPH6tznq6++iry8PMMjPT3drN+BiIiI5NMsZl3degdZIYRhWd++faHXm345bI1GA42GV14iIiKyBRbdouPl5QWlUmlovamWlZVVo5WHiIiI6FYWHXTUajUiIiIQHx9vtDw+Ph59+vS5q33HxcUhODgYkZGRd7UfIiIislyyd10VFhbi3LlzhtcpKSk4cuQIPDw8EBAQgDlz5mDixIno0aMHoqKisGzZMqSlpWH69Ol39bkxMTGIiYkxjNomIiIi6yN70Dl06BAGDhxoeD1nzhwAwOTJk7Fy5UqMHz8eOTk5eOutt5CRkYGQkBBs2rQJbdq0katkIiIiaiYs6jo6cjB1Hj4RERFZDlN/v2Vv0ZFbdc7Lz8+XuRIiIiIyVfXv9u3aa2w+6BQUFAAALxxIRETUDBUUFNQ71tbmu670ej0uX74MFxeXGtfruRvVt5ZIT09nl5gF4PmwPDwnloXnw7LwfNyeEAIFBQXw8/ODQlH3JHKbb9FRKBRo3bq12fbv6urKf0gtCM+H5eE5sSw8H5aF56N+psyatujr6BARERHdDQYdIiIisloMOmai0Wjwxhtv8L5aFoLnw/LwnFgWng/LwvPReGx+MDIRERFZL7boEBERkdVi0CEiIiKrxaBDREREVotBh4iIiKwWg46ZLF68GEFBQbC3t0dERAR2794td0nN3q5duzB69Gj4+flBkiRs2LDBaL0QAvPnz4efnx8cHBwwYMAAnDhxwmibsrIyPPvss/Dy8oKTkxPGjBmDS5cuGW2Tm5uLiRMnQqvVQqvVYuLEibh+/bqZv13zExsbi8jISLi4uMDb2xtjx47F6dOnjbbhOWk6S5YsQWhoqOECc1FRUdi8ebNhPc+FvGJjYyFJEmbPnm1YxnPSRAQ1ujVr1giVSiU+//xzkZycLGbNmiWcnJzExYsX5S6tWdu0aZOYO3eu+OGHHwQAsX79eqP17733nnBxcRE//PCDOHbsmBg/frxo2bKlyM/PN2wzffp00apVKxEfHy8OHz4sBg4cKMLCwkRlZaVhmwceeECEhISIvXv3ir1794qQkBAxatSopvqazcb9998vVqxYIY4fPy6OHDkiRo4cKQICAkRhYaFhG56TprNx40bx888/i9OnT4vTp0+L1157TahUKnH8+HEhBM+FnA4ePCgCAwNFaGiomDVrlmE5z0nTYNAxg549e4rp06cbLevUqZN45ZVXZKrI+twadPR6vfD19RXvvfeeYVlpaanQarVi6dKlQgghrl+/LlQqlVizZo1hmz///FMoFAqxZcsWIYQQycnJAoDYv3+/YZt9+/YJAOLUqVNm/lbNW1ZWlgAgdu7cKYTgObEE7u7u4osvvuC5kFFBQYG45557RHx8vOjfv78h6PCcNB12XTWy8vJyJCYmYtiwYUbLhw0bhr1798pUlfVLSUnBlStXjI67RqNB//79Dcc9MTERFRUVRtv4+fkhJCTEsM2+ffug1WrRq1cvwza9e/eGVqvl+buNvLw8AICHhwcAnhM56XQ6rFmzBkVFRYiKiuK5kFFMTAxGjhyJIUOGGC3nOWk6Nn9Tz8aWnZ0NnU4HHx8fo+U+Pj64cuWKTFVZv+pjW9txv3jxomEbtVoNd3f3GttUv//KlSvw9vausX9vb2+ev3oIITBnzhz07dsXISEhAHhO5HDs2DFERUWhtLQUzs7OWL9+PYKDgw0/eDwXTWvNmjU4fPgwEhISaqzjvx9Nh0HHTCRJMnothKixjBrfnRz3W7epbXuev/o988wzOHr0KPbs2VNjHc9J0+nYsSOOHDmC69ev44cffsDkyZOxc+dOw3qei6aTnp6OWbNmYevWrbC3t69zO54T82PXVSPz8vKCUqmskaSzsrJqJHdqPL6+vgBQ73H39fVFeXk5cnNz690mMzOzxv6vXr3K81eHZ599Fhs3bsT27dvRunVrw3Kek6anVqvRvn179OjRA7GxsQgLC8OiRYt4LmSQmJiIrKwsREREwM7ODnZ2dti5cyc++eQT2NnZGY4Xz4n5Meg0MrVajYiICMTHxxstj4+PR58+fWSqyvoFBQXB19fX6LiXl5dj586dhuMeEREBlUpltE1GRgaOHz9u2CYqKgp5eXk4ePCgYZsDBw4gLy+P5+8WQgg888wzWLduHbZt24agoCCj9Twn8hNCoKysjOdCBoMHD8axY8dw5MgRw6NHjx54/PHHceTIEbRt25bnpKk0/fhn61c9vfzLL78UycnJYvbs2cLJyUmkpqbKXVqzVlBQIJKSkkRSUpIAIBYsWCCSkpIM0/bfe+89odVqxbp168SxY8fEhAkTap2q2bp1a/Hrr7+Kw4cPi0GDBtU6VTM0NFTs27dP7Nu3T3Tt2pVTNWsxY8YModVqxY4dO0RGRobhUVxcbNiG56TpvPrqq2LXrl0iJSVFHD16VLz22mtCoVCIrVu3CiF4LizBzbOuhOA5aSoMOmYSFxcn2rRpI9Rqtejevbthyi3due3btwsANR6TJ08WQlRN13zjjTeEr6+v0Gg04r777hPHjh0z2kdJSYl45plnhIeHh3BwcBCjRo0SaWlpRtvk5OSIxx9/XLi4uAgXFxfx+OOPi9zc3Cb6ls1HbecCgFixYoVhG56TpjNlyhTDf3NatGghBg8ebAg5QvBcWIJbgw7PSdOQhBBCnrYkIiIiIvPiGB0iIiKyWgw6REREZLUYdIiIiMhqMegQERGR1WLQISIiIqvFoENERERWi0GHiIiIrBaDDhHZnMDAQCxcuFDuMoioCTDoEJFZRUdHY+zYsQCAAQMGYPbs2U322StXroSbm1uN5QkJCXj66aebrA4iko+d3AUQETVUeXk51Gr1Hb+/RYsWjVgNEVkytugQUZOIjo7Gzp07sWjRIkiSBEmSkJqaCgBITk7GiBEj4OzsDB8fH0ycOBHZ2dmG9w4YMADPPPMM5syZAy8vLwwdOhQAsGDBAnTt2hVOTk7w9/fHzJkzUVhYCADYsWMHnnzySeTl5Rk+b/78+QBqdl2lpaXhL3/5C5ydneHq6opHH30UmZmZhvXz589HeHg4Vq9ejcDAQGi1Wjz22GMoKCgwbPOf//wHXbt2hYODAzw9PTFkyBAUFRWZ6WgSkakYdIioSSxatAhRUVF46qmnkJGRgYyMDPj7+yMjIwP9+/dHeHg4Dh06hC1btiAzMxOPPvqo0ftXrVoFOzs7/P777/i///s/AIBCocAnn3yC48ePY9WqVdi2bRteeuklAECfPn2wcOFCuLq6Gj7vxRdfrFGXEAJjx47FtWvXsHPnTsTHx+P8+fMYP3680Xbnz5/Hhg0b8NNPP+Gnn37Czp078d577wEAMjIyMGHCBEyZMgUnT57Ejh078NBDD4G3EiSSH7uuiKhJaLVaqNVqODo6wtfX17B8yZIl6N69O/75z38ali1fvhz+/v44c+YMOnToAABo3749PvjgA6N93jzeJygoCG+//TZmzJiBxYsXQ61WQ6vVQpIko8+71a+//oqjR48iJSUF/v7+AIDVq1ejS5cuSEhIQGRkJABAr9dj5cqVcHFxAQBMnDgRv/32G959911kZGSgsrISDz30ENq0aQMA6Nq1610cLSJqLGzRISJZJSYmYvv27XB2djY8OnXqBKCqFaVajx49arx3+/btGDp0KFq1agUXFxdMmjQJOTk5DeoyOnnyJPz9/Q0hBwCCg4Ph5uaGkydPGpYFBgYaQg4AtGzZEllZWQCAsLAwDB48GF27dsW4cePw+eefIzc31/SDQERmw6BDRLLS6/UYPXo0jhw5YvQ4e/Ys7rvvPsN2Tk5ORu+7ePEiRowYgZCQEPzwww9ITExEXFwcAKCiosLkzxdCQJKk2y5XqVRG6yVJgl6vBwAolUrEx8dj8+bNCA4OxqeffoqOHTsiJSXF5DqIyDwYdIioyajVauh0OqNl3bt3x4kTJxAYGIj27dsbPW4NNzc7dOgQKisr8dFHH6F3797o0KEDLl++fNvPu1VwcDDS0tKQnp5uWJacnIy8vDx07tzZ5O8mSRLuvfdevPnmm0hKSoJarcb69etNfj8RmQeDDhE1mcDAQBw4cACpqanIzs6GXq9HTEwMrl27hgkTJuDgwYO4cOECtm7diilTptQbUtq1a4fKykp8+umnuHDhAlavXo2lS5fW+LzCwkL89ttvyM7ORnFxcY39DBkyBKGhoXj88cdx+PBhHDx4EJMmTUL//v1r7S6rzYEDB/DPf/4Thw4dQlpaGtatW4erV682KCgRkXkw6BBRk3nxxRehVCoRHByMFi1aIC0tDX5+fvj999+h0+lw//33IyQkBLNmzYJWq4VCUfd/osLDw7FgwQK8//77CAkJwddff43Y2Fijbfr06YPp06dj/PjxaNGiRY3BzEBVS8yGDRvg7u6O++67D0OGDEHbtm2xdu1ak7+Xq6srdu3ahREjRqBDhw6YN28ePvroIwwfPtz0g0NEZiEJzn8kIiIiK8UWHSIiIrJaDDpERERktRh0iIiIyGox6BAREZHVYtAhIiIiq8WgQ0RERFaLQYeIiIisFoMOERERWS0GHSIiIrJaDDpERERktRh0iIiIyGox6BAREZHV+n9Ja6fxF72urAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize parameters\n",
    "alpha = 3.6e-07                             # learning rate       \n",
    "max_iter = 4500                            # number of iterations\n",
    "\n",
    "# conduct gradient descent to obtain a multivariate linear regression model + loss function\n",
    "_, _, MSE = multi_gradient_descent_MSE(X_train, y_train, alpha, max_iter)\n",
    "plt.plot(range(1, max_iter+1), MSE)\n",
    "plt.title('Loss Function vs Iterations of GD')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Training MSE')\n",
    "plt.yscale('log')\n",
    "plt.savefig('lossoveriterations.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9bba69-49cf-4b57-8871-b9435802746f",
   "metadata": {},
   "source": [
    "# EXPORT TO CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38f4a7a5-9c51-4498-b2ce-bab16639ce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### UNIVARIATE MODEL RESULTS\n",
    "uvraw = np.vstack((m_uvraw, b_uvraw, train_MSE_uvraw, train_VE_uvraw, test_MSE_uvraw, test_VE_uvraw)).transpose()\n",
    "uvraw_df = pd.DataFrame(uvraw, columns=['m (slope)', 'b (intercept)', 'Training MSE', 'Training VE', 'Testing MSE', 'Testing VE'])\n",
    "\n",
    "uvnorm = np.hstack((m_uvnorm, b_uvnorm, train_MSE_uvnorm, train_VE_uvnorm, test_MSE_uvnorm, test_VE_uvnorm))\n",
    "uvnorm_df = pd.DataFrame(uvnorm, columns=['m (slope)', 'b (intercept)', 'Training MSE', 'Training VE', 'Testing MSE', 'Testing VE'])\n",
    "\n",
    "uvraw_df.to_csv(\"univariaterawresults.csv\")\n",
    "uvnorm_df.to_csv(\"univariatenormresults.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7701670-459a-4dd2-b94f-54ea5bfd1e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MULTIVARIATE MODEL RESULTS\n",
    "mvraw = np.vstack((m_mvraw, b_mvraw, train_MSE_mvraw, train_VE_mvraw, test_MSE_mvraw, test_VE_mvraw))\n",
    "mvnorm = np.vstack((m_mvnorm, b_mvnorm, train_MSE_mvnorm, train_VE_mvnorm, test_MSE_mvnorm, test_VE_mvnorm))\n",
    "\n",
    "mv = np.hstack((mvraw, mvnorm))\n",
    "mv_df = pd.DataFrame(mv, columns=['Raw Predictors', 'Normalized Predictors'])\n",
    "\n",
    "mv_df.to_csv(\"multivariateresults.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
